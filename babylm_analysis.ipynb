{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from evaluate_tasks import load_vl_data\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for embeddings.cls_token: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for embeddings.position_embeddings: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for embeddings.patch_embeddings.projection.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for embeddings.patch_embeddings.projection.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.0.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.1.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.2.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.3.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.4.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.5.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.6.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.7.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.8.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.9.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.10.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.attention.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.attention.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.attention.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.attention.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.attention.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.attention.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.layernorm_before.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.layernorm_before.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.layernorm_after.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for encoder.layer.11.layernorm_after.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:2047: UserWarning: for layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitAttention'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitLayer'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.vit.modeling_vit.ViTSdpaAttention'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.vit.modeling_vit.ViTLayer'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers_modules.modeling_git.GitForCausalLM'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/\", torch_dtype=t.float16,\n",
    "                      device_map=\"cuda\")\n",
    "hidden_size = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"EleutherAI/pythia-70m-deduped\", torch_dtype=t.float16,\n",
    "                      device_map=\"cuda\")\n",
    "hidden_size = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GitForCausalLM(\n",
       "  (git): GitModel(\n",
       "    (embeddings): GitEmbeddings(\n",
       "      (word_embeddings): Embedding(32778, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): GitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x GitLayer(\n",
       "          (attention): GitAttention(\n",
       "            (self): GitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): GitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (image_encoder): ViTModel(\n",
       "      (embeddings): ViTEmbeddings(\n",
       "        (patch_embeddings): ViTPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): ViTEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ViTLayer(\n",
       "            (attention): ViTSdpaAttention(\n",
       "              (attention): ViTSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): ViTPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (visual_projection): GitProjection(\n",
       "      (visual_projection): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodules = {}\n",
    "dictionaries = {}\n",
    "for idx, layer in enumerate(model.git.encoder.layer):\n",
    "    submodules[f\"mlp.{idx}\"] = layer.intermediate    # output of MLP\n",
    "    dictionaries[submodules[f\"mlp.{idx}\"]] = IdentityDict(hidden_size)\n",
    "    submodules[f\"attn.{idx}\"] = layer.attention  # output of attention\n",
    "    dictionaries[submodules[f\"attn.{idx}\"]] = IdentityDict(hidden_size)\n",
    "    submodules[f\"resid.{idx}\"] = layer      # output of whole layer\n",
    "    dictionaries[submodules[f\"resid.{idx}\"]] = IdentityDict(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[52,  1]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     56\u001b[0m pad_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m---> 58\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[43mload_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mignore_patch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 43\u001b[0m, in \u001b[0;36mload_examples\u001b[0;34m(dataset, num_examples, model, seed, pad_to_length, length, ignore_patch)\u001b[0m\n\u001b[1;32m     38\u001b[0m     patch_prefix \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mflip(F\u001b[38;5;241m.\u001b[39mpad(t\u001b[38;5;241m.\u001b[39mflip(patch_prefix, (\u001b[38;5;241m1\u001b[39m,)), (\u001b[38;5;241m0\u001b[39m, pad_length), value\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id), (\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_answer)\n\u001b[1;32m     41\u001b[0m example_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m: clean_prefix,\n\u001b[1;32m     42\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m: patch_prefix,\n\u001b[0;32m---> 43\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mclean_answer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     44\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: patch_answer\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     45\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix_length_wo_pad\u001b[39m\u001b[38;5;124m\"\u001b[39m: prefix_length_wo_pad,}\n\u001b[1;32m     46\u001b[0m examples\u001b[38;5;241m.\u001b[39mappend(example_dict)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "def load_examples(dataset, num_examples, model, seed=12, pad_to_length=None, length=None,\n",
    "                  ignore_patch=False):\n",
    "    examples = []\n",
    "    dataset_items = open(dataset).readlines()\n",
    "\n",
    "    for line in dataset_items:\n",
    "        data = json.loads(line)\n",
    "        clean_prefix = model.tokenizer(data[\"clean_prefix\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        patch_prefix = model.tokenizer(data[\"patch_prefix\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        clean_answer = model.tokenizer(data[\"clean_answer\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        patch_answer = model.tokenizer(data[\"patch_answer\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        # remove BOS tokens from answers\n",
    "        clean_answer = clean_answer[clean_answer != model.tokenizer.bos_token_id].unsqueeze(0)\n",
    "        patch_answer = patch_answer[patch_answer != model.tokenizer.bos_token_id].unsqueeze(0)\n",
    "        # only keep examples where answers are single tokens\n",
    "        if not ignore_patch:\n",
    "            if clean_prefix.shape[1] != patch_prefix.shape[1]:\n",
    "                continue\n",
    "        # only keep examples where clean and patch inputs are the same length\n",
    "        if clean_answer.shape[1] != 1 or patch_answer.shape[1] != 1:\n",
    "            continue\n",
    "        # if we specify a `length`, filter examples if they don't match\n",
    "        if length and clean_prefix.shape[1] != length:\n",
    "            continue\n",
    "        # if we specify `pad_to_length`, left-pad all inputs to a max length\n",
    "        prefix_length_wo_pad = clean_prefix.shape[1]\n",
    "        if pad_to_length:\n",
    "            model.tokenizer.padding_side = 'right'\n",
    "            pad_length = pad_to_length - prefix_length_wo_pad\n",
    "            if pad_length < 0:  # example too long\n",
    "                continue\n",
    "            # left padding: reverse, right-pad, reverse\n",
    "            clean_prefix = t.flip(F.pad(t.flip(clean_prefix, (1,)), (0, pad_length), value=model.tokenizer.pad_token_id), (1,))\n",
    "            patch_prefix = t.flip(F.pad(t.flip(patch_prefix, (1,)), (0, pad_length), value=model.tokenizer.pad_token_id), (1,))\n",
    "        \n",
    "        print(clean_answer)\n",
    "        example_dict = {\"clean_prefix\": clean_prefix,\n",
    "                        \"patch_prefix\": patch_prefix,\n",
    "                        \"clean_answer\": clean_answer.item(),\n",
    "                        \"patch_answer\": patch_answer.item(),\n",
    "                        \"prefix_length_wo_pad\": prefix_length_wo_pad,}\n",
    "        examples.append(example_dict)\n",
    "        if len(examples) >= num_examples:\n",
    "            break\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "data_path = \"data/simple_subject_verb_agreement.json\"\n",
    "ignore_patch = True\n",
    "num_examples = 100\n",
    "pad_length = 32\n",
    "\n",
    "examples = load_examples(data_path, num_examples, model, pad_to_length=pad_length,\n",
    "                                     ignore_patch=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "100%|| 32/32 [00:00<00:00, 182.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def load_vqa_examples(model, pad_to_length, n_samples):\n",
    "    samples = load_vl_data(task=\"vqa\", n_samples=n_samples)\n",
    "    examples = []\n",
    "    for sample_id, sample in samples.items():\n",
    "        clean_prefix = model.tokenizer(sample[\"question\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        clean_answer = model.tokenizer(sample[\"multiple_choice_answer\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        # remove BOS tokens from answers\n",
    "        clean_answer = clean_answer[clean_answer != model.tokenizer.bos_token_id].unsqueeze(0)\n",
    "        # TBD: skipping\n",
    "        # only keep examples where answers are single tokens\n",
    "        if clean_answer.shape[1] != 1:\n",
    "            continue\n",
    "        # only keep examples where clean and patch inputs are the same length\n",
    "\n",
    "        # if we specify `pad_to_length`, left-pad all inputs to a max length\n",
    "        prefix_length_wo_pad = clean_prefix.shape[1]\n",
    "        if pad_to_length:\n",
    "            model.tokenizer.padding_side = 'right'\n",
    "            pad_length = pad_to_length - prefix_length_wo_pad\n",
    "            if pad_length < 0:  # example too long\n",
    "                continue\n",
    "            # left padding: reverse, right-pad, reverse\n",
    "            clean_prefix = t.flip(F.pad(t.flip(clean_prefix, (1,)), (0, pad_length), value=model.tokenizer.pad_token_id), (1,))\n",
    "\n",
    "        example_dict = {\"clean_prefix\": clean_prefix,\n",
    "                            \"clean_answer\": clean_answer, # .item()\n",
    "                            \"question_type\": sample[\"question_type\"],\n",
    "                            \"prefix_length_wo_pad\": prefix_length_wo_pad,\n",
    "                            \"image\": sample[\"image\"]}\n",
    "        examples.append(example_dict)\n",
    "        if len(examples) >= n_samples:\n",
    "            break\n",
    "        \n",
    "    return examples\n",
    "\n",
    "\n",
    "vqa_examples = load_vqa_examples(model, pad_to_length=32, n_samples=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean_prefix': tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,  38,  27,   9, 169, 205,  23,\n",
       "            9, 753,  17,   1]]),\n",
       " 'clean_answer': tensor([[[[1228,    1]]]]),\n",
       " 'question_type': 'what is the man',\n",
       " 'prefix_length_wo_pad': 10,\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight\n",
    "#nnsight_model = NNsight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "model_path = \"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/\"\n",
    "nnsight_model = NNsight(model_path,\n",
    "                      device_map=\"auto\")\n",
    "img_processor = AutoProcessor.from_pretrained(\n",
    "                        model_path,\n",
    "                        trust_remote_code=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GitModel' object has no attribute 'git'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m img_processor(images\u001b[38;5;241m=\u001b[39msample1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mconvert(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nnsight_model\u001b[38;5;241m.\u001b[39mtrace(sample1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m], pixel_values\u001b[38;5;241m=\u001b[39mpixel_values, scan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 5\u001b[0m     logits \u001b[38;5;241m=\u001b[39m nnsight_model\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#print(logits.value)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/contexts/Runner.py:49\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/contexts/Tracer.py:71\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batched_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo input was provided to the tracing context.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mtracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:233\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Loads and dispatched ._model if not already done so.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatched:\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m intervention_graph\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:267\u001b[0m, in \u001b[0;36mNNsight.dispatch_model\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch ._model to have real parameters  using ._load(...).\"\"\"\u001b[39;00m\n\u001b[1;32m    265\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDispatching `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:335\u001b[0m, in \u001b[0;36mNNsight._load\u001b[0;34m(self, repo_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(repo_id, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AutoModel\u001b[38;5;241m.\u001b[39mfrom_config(config, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccelerate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint_and_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/accelerate/big_modeling.py:545\u001b[0m, in \u001b[0;36mload_checkpoint_and_dispatch\u001b[0;34m(model, checkpoint, device_map, max_memory, no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    544\u001b[0m     offload_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43mload_checkpoint_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/accelerate/utils/modeling.py:1494\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[0;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb)\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 offload_weight(param, param_name, state_dict_folder, index\u001b[38;5;241m=\u001b[39mstate_dict_index)\n\u001b[1;32m   1493\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1494\u001b[0m             \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m                \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m                \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# Force Python to clean up.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/accelerate/utils/modeling.py:269\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    267\u001b[0m splits \u001b[38;5;241m=\u001b[39m tensor_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 269\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GitModel' object has no attribute 'git'"
     ]
    }
   ],
   "source": [
    "sample1 = vqa_examples[0]\n",
    "pixel_values = img_processor(images=sample1[\"image\"].convert(mode=\"RGB\"), return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "with nnsight_model.trace(sample1[\"clean_prefix\"], pixel_values=pixel_values, scan=False, validate=False):\n",
    "    logits = nnsight_model.output\n",
    "\n",
    "#print(logits.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "# Attribution patching with integrated gradients\n",
    "def _pe_ig(\n",
    "        clean,\n",
    "        patch,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        steps=10,\n",
    "        metric_kwargs=dict(),\n",
    "):\n",
    "    \n",
    "    # first run through a test input to figure out which hidden states are tuples\n",
    "    is_tuple = {}\n",
    "    with model.trace(\"_\"):\n",
    "        for submodule in submodules:\n",
    "            is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "    # clean run -> model can be approximated through linear function of its activations\n",
    "    hidden_states_clean = {}\n",
    "    with model.trace(clean, **tracer_kwargs), t.no_grad():\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            f = dictionary.encode(x)\n",
    "            x_hat = dictionary.decode(f)\n",
    "            residual = x - x_hat\n",
    "            hidden_states_clean[submodule] = f.save()\n",
    "        metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "    hidden_states_clean = {k : v.value for k, v in hidden_states_clean.items()}\n",
    "\n",
    "    # corrupted run\n",
    "    if patch is None:\n",
    "        hidden_states_patch = {\n",
    "            k : t.zeros_like(v.act) for k, v in hidden_states_clean.items()\n",
    "        }\n",
    "        total_effect = None\n",
    "    else:\n",
    "        hidden_states_patch = {}\n",
    "        with model.trace(patch, **tracer_kwargs), t.no_grad():\n",
    "            for submodule in submodules:\n",
    "                dictionary = dictionaries[submodule]\n",
    "                x = submodule.output\n",
    "                if is_tuple[submodule]:\n",
    "                    x = x[0]\n",
    "                f = dictionary.encode(x)\n",
    "                x_hat = dictionary.decode(f)\n",
    "                residual = x - x_hat\n",
    "                hidden_states_patch[submodule] = f.save()\n",
    "            metric_patch = metric_fn(model, **metric_kwargs).save()\n",
    "        total_effect = (metric_patch.value - metric_clean.value).detach()\n",
    "        hidden_states_patch = {k : v.value for k, v in hidden_states_patch.items()}\n",
    "\n",
    "    effects = {}\n",
    "    deltas = {}\n",
    "    grads = {}\n",
    "    for submodule in submodules:\n",
    "        dictionary = dictionaries[submodule]\n",
    "        clean_state = hidden_states_clean[submodule]\n",
    "        patch_state = hidden_states_patch[submodule]\n",
    "        with model.trace(**tracer_kwargs) as tracer:\n",
    "            metrics = []\n",
    "            fs = []\n",
    "            for step in range(steps):\n",
    "                alpha = step / steps\n",
    "                f = (1 - alpha) * clean_state + alpha * patch_state\n",
    "                f.retain_grad()\n",
    "                fs.append(f)\n",
    "                with tracer.invoke(clean, scan=tracer_kwargs['scan']):\n",
    "                    if is_tuple[submodule]:\n",
    "                        submodule.output[0][:] = dictionary.decode(f)\n",
    "                    else:\n",
    "                        submodule.output = dictionary.decode(f)\n",
    "                    metrics.append(metric_fn(model, **metric_kwargs))\n",
    "            metric = sum([m for m in metrics])\n",
    "            metric.sum().backward(retain_graph=True) # TODO : why is this necessary? Probably shouldn't be, contact jaden\n",
    "\n",
    "        mean_grad = sum([f.grad for f in fs]) / steps\n",
    "        # mean_residual_grad = sum([f.grad for f in fs]) / steps\n",
    "        grad = mean_grad\n",
    "        delta = (patch_state - clean_state).detach() if patch_state is not None else -clean_state.detach()\n",
    "        effect = t.mul(grad, delta)\n",
    "\n",
    "        effects[submodule] = effect\n",
    "        deltas[submodule] = delta\n",
    "        grads[submodule] = grad\n",
    "\n",
    "    return (effects, deltas, grads, total_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|| 50/50 [02:58<00:00,  3.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive effects\n",
      "mlp_0\n",
      "tensor([335,  81,  17, 404, 127, 410, 195, 191, 248, 403], device='cuda:0')\n",
      "tensor([0.1709, 0.1609, 0.1528, 0.1281, 0.1266, 0.1201, 0.1144, 0.1092, 0.0952,\n",
      "        0.0927], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_1\n",
      "tensor([  6, 156, 132,  11, 508,  34, 121,  25, 366, 436], device='cuda:0')\n",
      "tensor([0.0906, 0.0804, 0.0784, 0.0691, 0.0662, 0.0516, 0.0513, 0.0474, 0.0469,\n",
      "        0.0466], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_2\n",
      "tensor([156, 499, 157, 260,   4,  52,  12,  11, 127,  49], device='cuda:0')\n",
      "tensor([0.2485, 0.0669, 0.0657, 0.0558, 0.0509, 0.0498, 0.0497, 0.0475, 0.0448,\n",
      "        0.0416], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_3\n",
      "tensor([111, 156,  91, 510, 117,  81, 508, 308, 351, 478], device='cuda:0')\n",
      "tensor([0.2539, 0.1236, 0.1036, 0.0912, 0.0862, 0.0801, 0.0790, 0.0773, 0.0635,\n",
      "        0.0583], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_4\n",
      "tensor([156,  23, 229, 478, 210, 329, 281, 299, 340, 122], device='cuda:0')\n",
      "tensor([0.1271, 0.0833, 0.0831, 0.0755, 0.0646, 0.0644, 0.0638, 0.0624, 0.0573,\n",
      "        0.0516], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_5\n",
      "tensor([229, 132, 477, 341, 299,  19, 389, 112, 400, 248], device='cuda:0')\n",
      "tensor([0.1281, 0.1207, 0.1096, 0.1025, 0.0959, 0.0821, 0.0820, 0.0796, 0.0771,\n",
      "        0.0770], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "negative effects\n",
      "mlp_0\n",
      "tensor([450, 478,  73, 508, 271, 193, 331, 390, 157, 207], device='cuda:0')\n",
      "tensor([-0.0698, -0.0561, -0.0520, -0.0494, -0.0455, -0.0450, -0.0400, -0.0324,\n",
      "        -0.0317, -0.0297], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_1\n",
      "tensor([ 17, 210, 157,  12,  63,  15, 203, 192,   5, 187], device='cuda:0')\n",
      "tensor([-0.0726, -0.0455, -0.0405, -0.0381, -0.0324, -0.0308, -0.0297, -0.0265,\n",
      "        -0.0260, -0.0249], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_2\n",
      "tensor([111, 224, 107, 478, 104, 324, 132, 436, 329,  71], device='cuda:0')\n",
      "tensor([-0.2749, -0.0695, -0.0659, -0.0561, -0.0530, -0.0522, -0.0464, -0.0413,\n",
      "        -0.0407, -0.0406], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_3\n",
      "tensor([329,   0, 260, 335, 477,   9, 281, 504, 172, 229], device='cuda:0')\n",
      "tensor([-0.0682, -0.0547, -0.0492, -0.0488, -0.0477, -0.0461, -0.0453, -0.0432,\n",
      "        -0.0384, -0.0368], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_4\n",
      "tensor([351,   4,  81, 157, 195, 462, 327, 283, 222,  91], device='cuda:0')\n",
      "tensor([-0.0576, -0.0374, -0.0353, -0.0346, -0.0310, -0.0260, -0.0246, -0.0244,\n",
      "        -0.0228, -0.0224], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "mlp_5\n",
      "tensor([131, 308, 210, 489, 302, 469, 206, 499, 178,  37], device='cuda:0')\n",
      "tensor([-0.1064, -0.1004, -0.0769, -0.0714, -0.0674, -0.0647, -0.0620, -0.0598,\n",
      "        -0.0579, -0.0561], device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Experiment hyperparameters\n",
    "batch_size = 2\n",
    "num_examples = 100\n",
    "device = \"cuda\"\n",
    "num_examples = min([num_examples, len(examples)])\n",
    "n_batches = math.ceil(len(examples) / batch_size)\n",
    "batches = [\n",
    "    examples[batch*batch_size:(batch+1)*batch_size] for batch in range(n_batches)\n",
    "]\n",
    "sum_effects = {}\n",
    "\n",
    "# Lists of submodules, separated by type\n",
    "resids = [submodules[submodule] for submodule in submodules if submodule.startswith(\"resid\")]\n",
    "mlps = [submodules[submodule] for submodule in submodules if submodule.startswith(\"mlp\")]\n",
    "attns = [submodules[submodule] for submodule in submodules if submodule.startswith(\"attn\")]\n",
    "\n",
    "# Loop through batches, run attribution patching\n",
    "for batch in tqdm(batches):\n",
    "    clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "    clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "\n",
    "    patch_answer_idxs = t.tensor([e['patch_answer'] for e in batch], dtype=t.long, device=device)\n",
    "    patch_inputs = t.cat([e['patch_prefix'] for e in batch], dim=0).to(device)\n",
    "    def metric_fn(model):\n",
    "        # This is basically `p(patch_answer) - p(clean_answer)`\n",
    "        return (\n",
    "            t.gather(model.embed_out.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "            t.gather(model.embed_out.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "        )\n",
    "\n",
    "    # Here, we're only looking at the MLP neurons\n",
    "    effects, _, _, _ = _pe_ig(\n",
    "        clean_inputs,\n",
    "        patch_inputs,\n",
    "        model,\n",
    "        mlps,\n",
    "        dictionaries,\n",
    "        metric_fn\n",
    "    )\n",
    "    for submodule in mlps:\n",
    "        if submodule not in sum_effects:\n",
    "            sum_effects[submodule] = effects[submodule].sum(dim=1).sum(dim=0)\n",
    "        else:\n",
    "            sum_effects[submodule] += effects[submodule].sum(dim=1).sum(dim=0)\n",
    "\n",
    "# Print top and bottom k neurons in each submodule\n",
    "k = 10\n",
    "\n",
    "print(\"positive effects\")\n",
    "for idx, submodule in enumerate(mlps):\n",
    "    sum_effects[submodule] /= num_examples\n",
    "    print(f\"mlp_{idx}\")\n",
    "    v, i = t.topk(sum_effects[submodule].flatten(), k)  # v=top effects, i=top indices\n",
    "    print(i)\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "print(\"negative effects\")\n",
    "for idx, submodule in enumerate(mlps):\n",
    "    print(f\"mlp_{idx}\")\n",
    "    v, i = t.topk(sum_effects[submodule].flatten(), k, largest=False)\n",
    "    print(i)\n",
    "    print(v)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
