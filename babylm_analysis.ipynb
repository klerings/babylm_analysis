{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "import torch as t\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from loading_utils import load_vqa_examples, load_blimp_examples, load_winoground_examples\n",
    "\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from nnsight import NNsight\n",
    "import importlib.util\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(own_model=True):\n",
    "    if own_model:\n",
    "        model_path = \"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/\"\n",
    "        spec = importlib.util.spec_from_file_location(\"GitForCausalLM\", f\"{model_path}modeling_git.py\")\n",
    "        git_module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[\"git_module\"] = git_module\n",
    "        spec.loader.exec_module(git_module)\n",
    "        GitForCausalLM = git_module.GitForCausalLM\n",
    "\n",
    "        model = GitForCausalLM.from_pretrained(model_path) \n",
    "        ckpt = torch.load(model_path + \"pytorch_model.bin\") # TODO: newly initialized for vision encoder: ['pooler.dense.bias', 'pooler.dense.weight']\n",
    "        model.load_state_dict(ckpt, strict=False)  \n",
    "        \n",
    "    else:\n",
    "        model_path = \"babylm/git-2024\"\n",
    "\n",
    "        from transformers import GitForCausalLM as OGModel\n",
    "\n",
    "        model = OGModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "    # load tokenizer and img processor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    img_processor = AutoProcessor.from_pretrained(model_path,trust_remote_code=True)\n",
    "    \n",
    "    nnsight_model = NNsight(model, device_map=\"cuda\")\n",
    "    nnsight_model.to(\"cuda\")\n",
    "\n",
    "    return nnsight_model, tokenizer, img_processor\n",
    "\n",
    "\n",
    "def extract_submodules(model):\n",
    "    submodules = {}\n",
    "    for idx, layer in enumerate(model.git.encoder.layer):\n",
    "        submodules[f\"mlp.{idx}\"] = layer.intermediate    # output of MLP\n",
    "        submodules[f\"attn.{idx}\"] = layer.attention  # output of attention\n",
    "        submodules[f\"resid.{idx}\"] = layer      # output of whole layer\n",
    "    return submodules\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of the model checkpoint at babylm/git-2024 were not used when initializing GitForCausalLM: ['git.image_encoder.embeddings.cls_token', 'git.image_encoder.embeddings.patch_embeddings.projection.bias', 'git.image_encoder.embeddings.patch_embeddings.projection.weight', 'git.image_encoder.embeddings.position_embeddings', 'git.image_encoder.encoder.layer.0.attention.attention.key.bias', 'git.image_encoder.encoder.layer.0.attention.attention.key.weight', 'git.image_encoder.encoder.layer.0.attention.attention.query.bias', 'git.image_encoder.encoder.layer.0.attention.attention.query.weight', 'git.image_encoder.encoder.layer.0.attention.attention.value.bias', 'git.image_encoder.encoder.layer.0.attention.attention.value.weight', 'git.image_encoder.encoder.layer.0.attention.output.dense.bias', 'git.image_encoder.encoder.layer.0.attention.output.dense.weight', 'git.image_encoder.encoder.layer.0.intermediate.dense.bias', 'git.image_encoder.encoder.layer.0.intermediate.dense.weight', 'git.image_encoder.encoder.layer.0.layernorm_after.bias', 'git.image_encoder.encoder.layer.0.layernorm_after.weight', 'git.image_encoder.encoder.layer.0.layernorm_before.bias', 'git.image_encoder.encoder.layer.0.layernorm_before.weight', 'git.image_encoder.encoder.layer.0.output.dense.bias', 'git.image_encoder.encoder.layer.0.output.dense.weight', 'git.image_encoder.encoder.layer.1.attention.attention.key.bias', 'git.image_encoder.encoder.layer.1.attention.attention.key.weight', 'git.image_encoder.encoder.layer.1.attention.attention.query.bias', 'git.image_encoder.encoder.layer.1.attention.attention.query.weight', 'git.image_encoder.encoder.layer.1.attention.attention.value.bias', 'git.image_encoder.encoder.layer.1.attention.attention.value.weight', 'git.image_encoder.encoder.layer.1.attention.output.dense.bias', 'git.image_encoder.encoder.layer.1.attention.output.dense.weight', 'git.image_encoder.encoder.layer.1.intermediate.dense.bias', 'git.image_encoder.encoder.layer.1.intermediate.dense.weight', 'git.image_encoder.encoder.layer.1.layernorm_after.bias', 'git.image_encoder.encoder.layer.1.layernorm_after.weight', 'git.image_encoder.encoder.layer.1.layernorm_before.bias', 'git.image_encoder.encoder.layer.1.layernorm_before.weight', 'git.image_encoder.encoder.layer.1.output.dense.bias', 'git.image_encoder.encoder.layer.1.output.dense.weight', 'git.image_encoder.encoder.layer.10.attention.attention.key.bias', 'git.image_encoder.encoder.layer.10.attention.attention.key.weight', 'git.image_encoder.encoder.layer.10.attention.attention.query.bias', 'git.image_encoder.encoder.layer.10.attention.attention.query.weight', 'git.image_encoder.encoder.layer.10.attention.attention.value.bias', 'git.image_encoder.encoder.layer.10.attention.attention.value.weight', 'git.image_encoder.encoder.layer.10.attention.output.dense.bias', 'git.image_encoder.encoder.layer.10.attention.output.dense.weight', 'git.image_encoder.encoder.layer.10.intermediate.dense.bias', 'git.image_encoder.encoder.layer.10.intermediate.dense.weight', 'git.image_encoder.encoder.layer.10.layernorm_after.bias', 'git.image_encoder.encoder.layer.10.layernorm_after.weight', 'git.image_encoder.encoder.layer.10.layernorm_before.bias', 'git.image_encoder.encoder.layer.10.layernorm_before.weight', 'git.image_encoder.encoder.layer.10.output.dense.bias', 'git.image_encoder.encoder.layer.10.output.dense.weight', 'git.image_encoder.encoder.layer.11.attention.attention.key.bias', 'git.image_encoder.encoder.layer.11.attention.attention.key.weight', 'git.image_encoder.encoder.layer.11.attention.attention.query.bias', 'git.image_encoder.encoder.layer.11.attention.attention.query.weight', 'git.image_encoder.encoder.layer.11.attention.attention.value.bias', 'git.image_encoder.encoder.layer.11.attention.attention.value.weight', 'git.image_encoder.encoder.layer.11.attention.output.dense.bias', 'git.image_encoder.encoder.layer.11.attention.output.dense.weight', 'git.image_encoder.encoder.layer.11.intermediate.dense.bias', 'git.image_encoder.encoder.layer.11.intermediate.dense.weight', 'git.image_encoder.encoder.layer.11.layernorm_after.bias', 'git.image_encoder.encoder.layer.11.layernorm_after.weight', 'git.image_encoder.encoder.layer.11.layernorm_before.bias', 'git.image_encoder.encoder.layer.11.layernorm_before.weight', 'git.image_encoder.encoder.layer.11.output.dense.bias', 'git.image_encoder.encoder.layer.11.output.dense.weight', 'git.image_encoder.encoder.layer.2.attention.attention.key.bias', 'git.image_encoder.encoder.layer.2.attention.attention.key.weight', 'git.image_encoder.encoder.layer.2.attention.attention.query.bias', 'git.image_encoder.encoder.layer.2.attention.attention.query.weight', 'git.image_encoder.encoder.layer.2.attention.attention.value.bias', 'git.image_encoder.encoder.layer.2.attention.attention.value.weight', 'git.image_encoder.encoder.layer.2.attention.output.dense.bias', 'git.image_encoder.encoder.layer.2.attention.output.dense.weight', 'git.image_encoder.encoder.layer.2.intermediate.dense.bias', 'git.image_encoder.encoder.layer.2.intermediate.dense.weight', 'git.image_encoder.encoder.layer.2.layernorm_after.bias', 'git.image_encoder.encoder.layer.2.layernorm_after.weight', 'git.image_encoder.encoder.layer.2.layernorm_before.bias', 'git.image_encoder.encoder.layer.2.layernorm_before.weight', 'git.image_encoder.encoder.layer.2.output.dense.bias', 'git.image_encoder.encoder.layer.2.output.dense.weight', 'git.image_encoder.encoder.layer.3.attention.attention.key.bias', 'git.image_encoder.encoder.layer.3.attention.attention.key.weight', 'git.image_encoder.encoder.layer.3.attention.attention.query.bias', 'git.image_encoder.encoder.layer.3.attention.attention.query.weight', 'git.image_encoder.encoder.layer.3.attention.attention.value.bias', 'git.image_encoder.encoder.layer.3.attention.attention.value.weight', 'git.image_encoder.encoder.layer.3.attention.output.dense.bias', 'git.image_encoder.encoder.layer.3.attention.output.dense.weight', 'git.image_encoder.encoder.layer.3.intermediate.dense.bias', 'git.image_encoder.encoder.layer.3.intermediate.dense.weight', 'git.image_encoder.encoder.layer.3.layernorm_after.bias', 'git.image_encoder.encoder.layer.3.layernorm_after.weight', 'git.image_encoder.encoder.layer.3.layernorm_before.bias', 'git.image_encoder.encoder.layer.3.layernorm_before.weight', 'git.image_encoder.encoder.layer.3.output.dense.bias', 'git.image_encoder.encoder.layer.3.output.dense.weight', 'git.image_encoder.encoder.layer.4.attention.attention.key.bias', 'git.image_encoder.encoder.layer.4.attention.attention.key.weight', 'git.image_encoder.encoder.layer.4.attention.attention.query.bias', 'git.image_encoder.encoder.layer.4.attention.attention.query.weight', 'git.image_encoder.encoder.layer.4.attention.attention.value.bias', 'git.image_encoder.encoder.layer.4.attention.attention.value.weight', 'git.image_encoder.encoder.layer.4.attention.output.dense.bias', 'git.image_encoder.encoder.layer.4.attention.output.dense.weight', 'git.image_encoder.encoder.layer.4.intermediate.dense.bias', 'git.image_encoder.encoder.layer.4.intermediate.dense.weight', 'git.image_encoder.encoder.layer.4.layernorm_after.bias', 'git.image_encoder.encoder.layer.4.layernorm_after.weight', 'git.image_encoder.encoder.layer.4.layernorm_before.bias', 'git.image_encoder.encoder.layer.4.layernorm_before.weight', 'git.image_encoder.encoder.layer.4.output.dense.bias', 'git.image_encoder.encoder.layer.4.output.dense.weight', 'git.image_encoder.encoder.layer.5.attention.attention.key.bias', 'git.image_encoder.encoder.layer.5.attention.attention.key.weight', 'git.image_encoder.encoder.layer.5.attention.attention.query.bias', 'git.image_encoder.encoder.layer.5.attention.attention.query.weight', 'git.image_encoder.encoder.layer.5.attention.attention.value.bias', 'git.image_encoder.encoder.layer.5.attention.attention.value.weight', 'git.image_encoder.encoder.layer.5.attention.output.dense.bias', 'git.image_encoder.encoder.layer.5.attention.output.dense.weight', 'git.image_encoder.encoder.layer.5.intermediate.dense.bias', 'git.image_encoder.encoder.layer.5.intermediate.dense.weight', 'git.image_encoder.encoder.layer.5.layernorm_after.bias', 'git.image_encoder.encoder.layer.5.layernorm_after.weight', 'git.image_encoder.encoder.layer.5.layernorm_before.bias', 'git.image_encoder.encoder.layer.5.layernorm_before.weight', 'git.image_encoder.encoder.layer.5.output.dense.bias', 'git.image_encoder.encoder.layer.5.output.dense.weight', 'git.image_encoder.encoder.layer.6.attention.attention.key.bias', 'git.image_encoder.encoder.layer.6.attention.attention.key.weight', 'git.image_encoder.encoder.layer.6.attention.attention.query.bias', 'git.image_encoder.encoder.layer.6.attention.attention.query.weight', 'git.image_encoder.encoder.layer.6.attention.attention.value.bias', 'git.image_encoder.encoder.layer.6.attention.attention.value.weight', 'git.image_encoder.encoder.layer.6.attention.output.dense.bias', 'git.image_encoder.encoder.layer.6.attention.output.dense.weight', 'git.image_encoder.encoder.layer.6.intermediate.dense.bias', 'git.image_encoder.encoder.layer.6.intermediate.dense.weight', 'git.image_encoder.encoder.layer.6.layernorm_after.bias', 'git.image_encoder.encoder.layer.6.layernorm_after.weight', 'git.image_encoder.encoder.layer.6.layernorm_before.bias', 'git.image_encoder.encoder.layer.6.layernorm_before.weight', 'git.image_encoder.encoder.layer.6.output.dense.bias', 'git.image_encoder.encoder.layer.6.output.dense.weight', 'git.image_encoder.encoder.layer.7.attention.attention.key.bias', 'git.image_encoder.encoder.layer.7.attention.attention.key.weight', 'git.image_encoder.encoder.layer.7.attention.attention.query.bias', 'git.image_encoder.encoder.layer.7.attention.attention.query.weight', 'git.image_encoder.encoder.layer.7.attention.attention.value.bias', 'git.image_encoder.encoder.layer.7.attention.attention.value.weight', 'git.image_encoder.encoder.layer.7.attention.output.dense.bias', 'git.image_encoder.encoder.layer.7.attention.output.dense.weight', 'git.image_encoder.encoder.layer.7.intermediate.dense.bias', 'git.image_encoder.encoder.layer.7.intermediate.dense.weight', 'git.image_encoder.encoder.layer.7.layernorm_after.bias', 'git.image_encoder.encoder.layer.7.layernorm_after.weight', 'git.image_encoder.encoder.layer.7.layernorm_before.bias', 'git.image_encoder.encoder.layer.7.layernorm_before.weight', 'git.image_encoder.encoder.layer.7.output.dense.bias', 'git.image_encoder.encoder.layer.7.output.dense.weight', 'git.image_encoder.encoder.layer.8.attention.attention.key.bias', 'git.image_encoder.encoder.layer.8.attention.attention.key.weight', 'git.image_encoder.encoder.layer.8.attention.attention.query.bias', 'git.image_encoder.encoder.layer.8.attention.attention.query.weight', 'git.image_encoder.encoder.layer.8.attention.attention.value.bias', 'git.image_encoder.encoder.layer.8.attention.attention.value.weight', 'git.image_encoder.encoder.layer.8.attention.output.dense.bias', 'git.image_encoder.encoder.layer.8.attention.output.dense.weight', 'git.image_encoder.encoder.layer.8.intermediate.dense.bias', 'git.image_encoder.encoder.layer.8.intermediate.dense.weight', 'git.image_encoder.encoder.layer.8.layernorm_after.bias', 'git.image_encoder.encoder.layer.8.layernorm_after.weight', 'git.image_encoder.encoder.layer.8.layernorm_before.bias', 'git.image_encoder.encoder.layer.8.layernorm_before.weight', 'git.image_encoder.encoder.layer.8.output.dense.bias', 'git.image_encoder.encoder.layer.8.output.dense.weight', 'git.image_encoder.encoder.layer.9.attention.attention.key.bias', 'git.image_encoder.encoder.layer.9.attention.attention.key.weight', 'git.image_encoder.encoder.layer.9.attention.attention.query.bias', 'git.image_encoder.encoder.layer.9.attention.attention.query.weight', 'git.image_encoder.encoder.layer.9.attention.attention.value.bias', 'git.image_encoder.encoder.layer.9.attention.attention.value.weight', 'git.image_encoder.encoder.layer.9.attention.output.dense.bias', 'git.image_encoder.encoder.layer.9.attention.output.dense.weight', 'git.image_encoder.encoder.layer.9.intermediate.dense.bias', 'git.image_encoder.encoder.layer.9.intermediate.dense.weight', 'git.image_encoder.encoder.layer.9.layernorm_after.bias', 'git.image_encoder.encoder.layer.9.layernorm_after.weight', 'git.image_encoder.encoder.layer.9.layernorm_before.bias', 'git.image_encoder.encoder.layer.9.layernorm_before.weight', 'git.image_encoder.encoder.layer.9.output.dense.bias', 'git.image_encoder.encoder.layer.9.output.dense.weight', 'git.image_encoder.layernorm.bias', 'git.image_encoder.layernorm.weight', 'git.image_encoder.pooler.dense.bias', 'git.image_encoder.pooler.dense.weight']\n",
      "- This IS expected if you are initializing GitForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GitForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GitForCausalLM were not initialized from the model checkpoint at babylm/git-2024 and are newly initialized: ['git.image_encoder.vision_model.embeddings.class_embedding', 'git.image_encoder.vision_model.embeddings.patch_embedding.weight', 'git.image_encoder.vision_model.embeddings.position_embedding.weight', 'git.image_encoder.vision_model.encoder.layers.0.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.0.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.0.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.0.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.0.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.0.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.0.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.0.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.1.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.1.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.1.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.1.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.1.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.1.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.1.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.1.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.10.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.10.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.10.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.10.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.10.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.10.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.10.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.10.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.11.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.11.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.11.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.11.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.11.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.11.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.11.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.11.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.12.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.12.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.12.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.12.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.12.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.12.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.12.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.12.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.13.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.13.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.13.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.13.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.13.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.13.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.13.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.13.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.14.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.14.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.14.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.14.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.14.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.14.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.14.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.14.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.15.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.15.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.15.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.15.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.15.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.15.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.15.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.15.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.16.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.16.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.16.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.16.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.16.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.16.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.16.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.16.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.17.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.17.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.17.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.17.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.17.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.17.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.17.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.17.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.18.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.18.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.18.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.18.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.18.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.18.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.18.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.18.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.19.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.19.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.19.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.19.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.19.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.19.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.19.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.19.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.2.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.2.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.2.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.2.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.2.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.2.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.2.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.2.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.20.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.20.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.20.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.20.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.20.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.20.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.20.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.20.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.21.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.21.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.21.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.21.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.21.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.21.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.21.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.21.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.22.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.22.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.22.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.22.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.22.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.22.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.22.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.22.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.23.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.23.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.23.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.23.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.23.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.23.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.23.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.23.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.3.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.3.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.3.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.3.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.3.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.3.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.3.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.3.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.4.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.4.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.4.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.4.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.4.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.4.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.4.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.4.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.5.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.5.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.5.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.5.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.5.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.5.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.5.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.5.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.6.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.6.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.6.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.6.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.6.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.6.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.6.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.6.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.7.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.7.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.7.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.7.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.7.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.7.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.7.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.7.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.8.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.8.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.8.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.8.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.8.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.8.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.8.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.8.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'git.image_encoder.vision_model.encoder.layers.9.layer_norm1.bias', 'git.image_encoder.vision_model.encoder.layers.9.layer_norm1.weight', 'git.image_encoder.vision_model.encoder.layers.9.layer_norm2.bias', 'git.image_encoder.vision_model.encoder.layers.9.layer_norm2.weight', 'git.image_encoder.vision_model.encoder.layers.9.mlp.fc1.bias', 'git.image_encoder.vision_model.encoder.layers.9.mlp.fc1.weight', 'git.image_encoder.vision_model.encoder.layers.9.mlp.fc2.bias', 'git.image_encoder.vision_model.encoder.layers.9.mlp.fc2.weight', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'git.image_encoder.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'git.image_encoder.vision_model.post_layernorm.bias', 'git.image_encoder.vision_model.post_layernorm.weight', 'git.image_encoder.vision_model.pre_layrnorm.bias', 'git.image_encoder.vision_model.pre_layrnorm.weight', 'output.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitAttention'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitLayer'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitForCausalLM'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load and prepare model\n",
    "model, tokenizer, img_processor = load_model(own_model=True)\n",
    "submodules = extract_submodules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for facebook/winoground contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/facebook/winoground\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded huggingface DS\n",
      "loaded local DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 22.24it/s]\n"
     ]
    }
   ],
   "source": [
    "winoground_examples = load_winoground_examples(tokenizer, img_processor, pad_to_length=32, n_samples=10, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,  310,  401,  114, 7434,   45,    5,    1]]),\n",
       " 'clean_answer': 1370,\n",
       " 'patch_prefix': tensor([[ 310,  401,  114, 7434,   45,    5,    1]]),\n",
       " 'patch_answer': 404,\n",
       " 'UID': 'anaphor_gender_agreement',\n",
       " 'linguistics_term': 'anaphor_agreement',\n",
       " 'prefix_length_wo_pad': 7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blimp_examples = load_blimp_examples(tokenizer, pad_to_length=32, n_samples=10)\n",
    "blimp_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded huggingface DS\n",
      "loaded local DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 169.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,   27,   44, 4045,   23,  463,   17,    1]]),\n",
       " 'clean_answer': 49,\n",
       " 'distractors': [3895, 1224, 121, 1017, 303, 55, 175],\n",
       " 'question_type': 'is this',\n",
       " 'prefix_length_wo_pad': 7,\n",
       " 'pixel_values': tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1008, -2.1179,  ..., -2.1008, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1008, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1008, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0182, -2.0357,  ..., -2.0182, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0182, -2.0357, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0182, -2.0357, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.7870, -1.7870,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.7870, -1.8044, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.7870, -1.8044, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.7870]]]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and prepare data\n",
    "vqa_examples = load_vqa_examples(tokenizer, img_processor, pad_to_length=32, n_samples=10, local=True)\n",
    "vqa_examples[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mean_activations(examples, model, submodules, batch_size, noimg=False, file_prefix=None):\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "    device = \"cuda\"\n",
    "    num_examples = len(examples)\n",
    "    batches = [\n",
    "        examples[i:min(i + batch_size,num_examples)] for i in range(0, num_examples, batch_size)\n",
    "    ]\n",
    "\n",
    "    def extract_hidden_states(submodule):\n",
    "        total_samples = 0\n",
    "        # Initialize storage for cumulative activations and count of samples\n",
    "        cumulative_activations = 0\n",
    "\n",
    "        for batch in tqdm(batches):\n",
    "            clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "        \n",
    "            # clean run -> model can be approximated through linear function of its activations\n",
    "            hidden_states_clean = {}\n",
    "            #with autocast():\n",
    "            if noimg:\n",
    "                with model.trace(clean_inputs, **tracer_kwargs), t.no_grad():\n",
    "                    x = submodule.output\n",
    "                    hidden_states_clean = x.save()\n",
    "            else:\n",
    "                img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "                with model.trace(clean_inputs, pixel_values=img_inputs, **tracer_kwargs), t.no_grad():\n",
    "                    x = submodule.output\n",
    "                    hidden_states_clean = x.save()\n",
    "            hidden_states_clean = hidden_states_clean.value\n",
    "\n",
    "            batch_size = clean_inputs.shape[0]  # Assuming shape [batch_size, ...]\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # Sum across the batch (dim=0)\n",
    "            cumulative_activations += hidden_states_clean.sum(dim=(0, 1)).detach().cpu()  # detach\n",
    "            \n",
    "            hidden_states_clean = None\n",
    "            clean_inputs = None\n",
    "            state = None\n",
    "            x = None\n",
    "            batch_size = None\n",
    "            del hidden_states_clean, clean_inputs, state, x, batch_size\n",
    "            if not noimg:\n",
    "                img_inputs = None\n",
    "                del img_inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Compute mean activation by dividing the cumulative activations by the total number of samples\n",
    "        mean_activations = cumulative_activations / total_samples\n",
    "\n",
    "        cumulative_activations = None\n",
    "        del cumulative_activations\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return mean_activations\n",
    "\n",
    "    mean_act_files = []\n",
    "    for i, submodule in enumerate(submodules):\n",
    "        submodule_acts = extract_hidden_states(submodule)\n",
    "        filename = f\"mean_activations/{file_prefix}_mean_acts_{i}_small.npy\"\n",
    "        np.save(filename, submodule_acts)\n",
    "\n",
    "        submodule_acts = None\n",
    "        del submodule_acts\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        mean_act_files.append(filename)\n",
    "\n",
    "    return mean_act_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution patching with integrated gradients\n",
    "def _pe_ig(\n",
    "        clean,\n",
    "        img_inputs,\n",
    "        model,\n",
    "        submodules,\n",
    "        mean_act_files,\n",
    "        metric_fn,\n",
    "        steps=10,\n",
    "        metric_kwargs=dict()):\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "    \n",
    "    # clean run -> model can be approximated through linear function of its activations\n",
    "    hidden_states_clean = {}\n",
    "    if img_inputs is None:\n",
    "        with model.trace(clean, **tracer_kwargs), t.no_grad():\n",
    "            for submodule in submodules:\n",
    "                x = submodule.output\n",
    "                hidden_states_clean[submodule] = x.save()\n",
    "            metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "    else:\n",
    "        with model.trace(clean, pixel_values=img_inputs, **tracer_kwargs), t.no_grad(): \n",
    "            for submodule in submodules:\n",
    "                x = submodule.output\n",
    "                hidden_states_clean[submodule] = x.save()\n",
    "            metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "    hidden_states_clean = {k : v.value for k, v in hidden_states_clean.items()}\n",
    "\n",
    "    x = None\n",
    "    del x\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    effects = {}\n",
    "    deltas = {}\n",
    "    grads = {}\n",
    "    for i, submodule in enumerate(submodules):\n",
    "        # load mean hidden states from file\n",
    "        mean_state = np.load(mean_act_files[i], allow_pickle=True)\n",
    "        mean_state = torch.tensor(mean_state).to(\"cuda\")\n",
    "        mean_state.requires_grad = True\n",
    "\n",
    "        clean_state = hidden_states_clean[submodule]\n",
    "\n",
    "        # computational graph without img: [batch_size, 32, n_dim]\n",
    "        # computational graph img: [batch_size, 229, n_dim]\n",
    "        with model.trace(**tracer_kwargs) as tracer:  # calling the trace() function without input determines the computational graph as one without images\n",
    "            metrics = []\n",
    "            fs = []\n",
    "            for step in range(steps):\n",
    "                alpha = step / steps\n",
    "                f = (1 - alpha) * clean_state + alpha * mean_state\n",
    "                f.retain_grad()\n",
    "                fs.append(f)\n",
    "                \n",
    "                if img_inputs is None:\n",
    "                    with tracer.invoke(clean, scan=tracer_kwargs['scan']):\n",
    "                        submodule.output = f\n",
    "                        metrics.append(metric_fn(model, **metric_kwargs))\n",
    "                else:\n",
    "                    with tracer.invoke(clean, pixel_values=img_inputs, scan=tracer_kwargs['scan']):\n",
    "                        submodule.output = f\n",
    "                        metrics.append(metric_fn(model, **metric_kwargs))\n",
    "            metric = sum([m for m in metrics])\n",
    "            metric.sum().backward(retain_graph=True) # TODO : why is this necessary? Probably shouldn't be, contact jaden\n",
    "        \n",
    "        mean_grad = sum([f.grad for f in fs]) / steps\n",
    "        # mean_residual_grad = sum([f.grad for f in fs]) / steps\n",
    "        grad = mean_grad\n",
    "        delta = (mean_state - clean_state).detach() if mean_state is not None else -clean_state.detach()\n",
    "        effect = t.mul(grad, delta)\n",
    "\n",
    "        effects[submodule] = effect\n",
    "        deltas[submodule] = delta\n",
    "        grads[submodule] = grad\n",
    "\n",
    "        mean_state = None\n",
    "        del mean_state\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    return (effects, deltas, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_neurons(examples, batch_size, mlps, mean_act_files, task):\n",
    "    # uses attribution patching to identify most important neurons for subtask\n",
    "    num_examples = len(examples)\n",
    "    batches = [examples[i:min(i + batch_size, num_examples)] for i in range(0, num_examples, batch_size)]\n",
    "    device = \"cuda\"\n",
    "\n",
    "    sum_effects = {}\n",
    "\n",
    "    for batch in tqdm(batches):\n",
    "        clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "        clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "\n",
    "        if task == \"vqa\":\n",
    "            img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "\n",
    "            first_distractor_idxs = t.tensor([e['distractors'][0] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "            def metric(model):\n",
    "                # compute difference between correct answer and first distractor\n",
    "                # TODO: compute avg difference between correct answer and each distractor\n",
    "                #embds_out = model.output.output.save()\n",
    "                \n",
    "                return (\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=first_distractor_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "                )\n",
    "        \n",
    "        elif task == \"blimp\":\n",
    "            img_inputs = None\n",
    "            \n",
    "            patch_answer_idxs = t.tensor([e['patch_answer'] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "            def metric(model):\n",
    "                \n",
    "                return (\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "                )\n",
    "        \n",
    "\n",
    "        effects, _, _ = _pe_ig(\n",
    "            clean_inputs,\n",
    "            img_inputs,\n",
    "            model,\n",
    "            mlps,\n",
    "            mean_act_files,\n",
    "            metric,\n",
    "            steps=10,\n",
    "            metric_kwargs=dict())\n",
    "        \n",
    "        \n",
    "        for submodule in mlps:\n",
    "            if submodule not in sum_effects:\n",
    "                sum_effects[submodule] = effects[submodule].sum(dim=1).sum(dim=0)\n",
    "            else:\n",
    "                sum_effects[submodule] += effects[submodule].sum(dim=1).sum(dim=0)\n",
    "\n",
    "    # Print top 100 neurons in each submodule (ndim=3072)\n",
    "    k = 100\n",
    "\n",
    "    top_neurons = {}\n",
    "    for idx, submodule in enumerate(mlps):\n",
    "        sum_effects[submodule] /= num_examples\n",
    "        v, i = t.topk(sum_effects[submodule].flatten(), k)  # v=top effects, i=top indices\n",
    "        top_neurons[f\"mlp_{idx}\"] = (i,v)\n",
    "    return top_neurons\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  #16\n",
    "num_examples = 8  #-1\n",
    "task = \"vqa\"\n",
    "model_name = \"git_1vd125_s1\"\n",
    "epoch = 23\n",
    "local = True\n",
    "\n",
    "def run_task(task):\n",
    "    mlps = [submodules[submodule] for submodule in submodules if submodule.startswith(\"mlp\")]\n",
    "    mlps = mlps[:2]\n",
    "\n",
    "    noimg = False\n",
    "\n",
    "    # load and prepare data\n",
    "    if task == \"vqa\":\n",
    "        examples = load_vqa_examples(tokenizer, img_processor, pad_to_length=32, n_samples=num_examples, local=local)\n",
    "        subtask_key = \"question_type\"\n",
    "    elif task == \"blimp\":\n",
    "        noimg = True\n",
    "        examples = load_blimp_examples(tokenizer, pad_to_length=32, n_samples=num_examples)\n",
    "        subtask_key = \"linguistics_term\"\n",
    "    else:\n",
    "        print(f\"{task} is not implemented\")\n",
    "    print(\"loaded samples\")\n",
    "\n",
    "    # precompute mean activations on task or retrieve precomputed activation files\n",
    "    prefix = f\"{task}_{model_name}\"\n",
    "    mean_act_files = []\n",
    "    for file in os.listdir(\"mean_activations/\"):\n",
    "        if file.startswith(prefix) and file.endswith(\"small.npy\"):\n",
    "            mean_act_files.append(f\"mean_activations/{file}\")\n",
    "    if len(mean_act_files) == 0:\n",
    "        mean_act_files = compute_mean_activations(examples, model, mlps, batch_size=128, noimg=noimg, file_prefix=prefix)\n",
    "\n",
    "    print(f\"computed mean activations\")\n",
    "\n",
    "    # identify subtasks\n",
    "    subtasks = {}\n",
    "    for e in examples:\n",
    "        subtask = e[subtask_key]\n",
    "        if subtask in subtasks:\n",
    "            subtasks[subtask].append(e)\n",
    "        else:\n",
    "            subtasks[subtask] = [e]\n",
    "\n",
    "    print(\"extracted subtasks\")\n",
    "\n",
    "    # for each subtask, compute top neurons and save\n",
    "    subtasks_neurons = {}\n",
    "    for subtask, examples in subtasks.items():\n",
    "        top_neurons = get_important_neurons(examples, batch_size, mlps, mean_act_files, task=task)\n",
    "        subtasks_neurons[subtask] = top_neurons\n",
    "        print(f\"finished subtask: {subtask}\")\n",
    "\n",
    "    with open(f\"data/{model_name}_e{epoch}_{task}_top_neurons_per_subtask.pkl\", \"wb\") as f:\n",
    "        pickle.dump(subtasks_neurons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded huggingface DS\n",
      "loaded local DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 250.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded samples\n",
      "computed mean activations\n",
      "extracted subtasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 32 but got size 289 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvqa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mrun_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     49\u001b[0m subtasks_neurons \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subtask, examples \u001b[38;5;129;01min\u001b[39;00m subtasks\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 51\u001b[0m     top_neurons \u001b[38;5;241m=\u001b[39m \u001b[43mget_important_neurons\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_act_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     subtasks_neurons[subtask] \u001b[38;5;241m=\u001b[39m top_neurons\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished subtask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mget_important_neurons\u001b[0;34m(examples, batch_size, mlps, mean_act_files, task)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetric\u001b[39m(model):\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     36\u001b[0m             t\u001b[38;5;241m.\u001b[39mgather(model\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39moutput[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mpatch_answer_idxs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m     37\u001b[0m             t\u001b[38;5;241m.\u001b[39mgather(model\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39moutput[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mclean_answer_idxs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m         )\n\u001b[0;32m---> 41\u001b[0m effects, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_pe_ig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmean_act_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m mlps:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m submodule \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sum_effects:\n",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m, in \u001b[0;36m_pe_ig\u001b[0;34m(clean, img_inputs, model, submodules, mean_act_files, metric_fn, steps, metric_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 metrics\u001b[38;5;241m.\u001b[39mappend(metric_fn(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetric_kwargs))\n\u001b[1;32m     64\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics])\n\u001b[0;32m---> 65\u001b[0m     metric\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# TODO : why is this necessary? Probably shouldn't be, contact jaden\u001b[39;00m\n\u001b[1;32m     67\u001b[0m mean_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([f\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs]) \u001b[38;5;241m/\u001b[39m steps\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# mean_residual_grad = sum([f.grad for f in fs]) / steps\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/contexts/Runner.py:49\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/contexts/Tracer.py:71\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batched_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo input was provided to the tracing context.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mtracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:253\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m     ),\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    255\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:452\u001b[0m, in \u001b[0;36mHookHandler.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    449\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:253\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m     ),\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:356\u001b[0m, in \u001b[0;36mNNsight._execute\u001b[0;34m(self, *prepared_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepared_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:1483\u001b[0m, in \u001b[0;36mGitForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1481\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1483\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1497\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1498\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:1275\u001b[0m, in \u001b[0;36mGitModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1273\u001b[0m         combined_attention_mask[:, :, \u001b[38;5;241m-\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m] :, \u001b[38;5;241m-\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m] :] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m expanded_attn_mask\n\u001b[0;32m-> 1275\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values_present\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:452\u001b[0m, in \u001b[0;36mGitEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, pixel_values_present, return_dict)\u001b[0m\n\u001b[1;32m    443\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    444\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    445\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m         output_attentions,\n\u001b[1;32m    450\u001b[0m     )\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values_present\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:388\u001b[0m, in \u001b[0;36mGitLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, past_key_value, output_attentions, pixel_values_present)\u001b[0m\n\u001b[1;32m    385\u001b[0m outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    386\u001b[0m present_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 388\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:399\u001b[0m, in \u001b[0;36mGitLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 399\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:439\u001b[0m, in \u001b[0;36mHookHandler.__enter__.<locals>.output_hook\u001b[0;34m(module, input, output, module_path)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutput_hook\u001b[39m(module, \u001b[38;5;28minput\u001b[39m, output, module_path\u001b[38;5;241m=\u001b[39mmodule_path):\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:249\u001b[0m, in \u001b[0;36mNNsight.interleave.<locals>.<lambda>\u001b[0;34m(activations, module_path)\u001b[0m\n\u001b[1;32m    239\u001b[0m inputs, total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    241\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m    246\u001b[0m     input_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m activations, module_path: intervene(\n\u001b[1;32m    247\u001b[0m         activations, module_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, intervention_handler\n\u001b[1;32m    248\u001b[0m     ),\n\u001b[0;32m--> 249\u001b[0m     output_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m activations, module_path: \u001b[43mintervene\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_handler\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    252\u001b[0m ):\n\u001b[1;32m    253\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    255\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:362\u001b[0m, in \u001b[0;36mintervene\u001b[0;34m(activations, module_path, key, intervention_handler)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# If we narrowed any data, we need to concat it with data before and after it.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m narrowed:\n\u001b[0;32m--> 362\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintervention_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Otherwise just return the whole value as the activations.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     activations \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:272\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(activations, value, batch_start, batch_size, total_batch_size)\u001b[0m\n\u001b[1;32m    264\u001b[0m post \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    265\u001b[0m     activations,\n\u001b[1;32m    266\u001b[0m     narrow2,\n\u001b[1;32m    267\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    268\u001b[0m )\n\u001b[1;32m    270\u001b[0m orig_sizes \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mapply(activations, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sizes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:227\u001b[0m, in \u001b[0;36mconcat.<locals>._concat\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    225\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_size \u001b[38;5;241m==\u001b[39m orig_size:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 32 but got size 289 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "run_task(\"vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
