{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "import torch as t\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from loading_utils import load_vqa_examples, load_blimp_examples, load_winoground_examples\n",
    "\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from nnsight import NNsight\n",
    "import importlib.util\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(own_model=True):\n",
    "    if own_model:\n",
    "        model_path = \"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/\"\n",
    "        spec = importlib.util.spec_from_file_location(\"GitForCausalLM\", f\"{model_path}modeling_git.py\")\n",
    "        git_module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[\"git_module\"] = git_module\n",
    "        spec.loader.exec_module(git_module)\n",
    "        GitForCausalLM = git_module.GitForCausalLM\n",
    "\n",
    "        model = GitForCausalLM.from_pretrained(model_path) \n",
    "        ckpt = torch.load(model_path + \"pytorch_model.bin\") # TODO: newly initialized for vision encoder: ['pooler.dense.bias', 'pooler.dense.weight']\n",
    "        model.load_state_dict(ckpt, strict=False)  \n",
    "        \n",
    "    else:\n",
    "        model_path = \"babylm/git-2024\"\n",
    "\n",
    "        from transformers import GitForCausalLM as OGModel\n",
    "\n",
    "        model = OGModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "    # load tokenizer and img processor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    img_processor = AutoProcessor.from_pretrained(model_path,trust_remote_code=True)\n",
    "    \n",
    "    nnsight_model = NNsight(model, device_map=\"cuda\")\n",
    "    nnsight_model.to(\"cuda\")\n",
    "\n",
    "    return nnsight_model, tokenizer, img_processor\n",
    "\n",
    "\n",
    "def extract_submodules(model):\n",
    "    submodules = {}\n",
    "    for idx, layer in enumerate(model.git.encoder.layer):\n",
    "        submodules[f\"mlp.{idx}\"] = layer.intermediate    # output of MLP\n",
    "        submodules[f\"attn.{idx}\"] = layer.attention  # output of attention\n",
    "        submodules[f\"resid.{idx}\"] = layer      # output of whole layer\n",
    "    return submodules\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitAttention'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitLayer'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.vit.modeling_vit.ViTSdpaAttention'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.vit.modeling_vit.ViTLayer'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'GitForCausalLM.GitForCausalLM'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load and prepare model\n",
    "model, tokenizer, img_processor = load_model(own_model=True)\n",
    "submodules = extract_submodules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for facebook/winoground contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/facebook/winoground\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded huggingface DS\n",
      "loaded local DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 22.24it/s]\n"
     ]
    }
   ],
   "source": [
    "winoground_examples = load_winoground_examples(tokenizer, img_processor, pad_to_length=32, n_samples=10, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,  310,  401,  114, 7434,   45,    5,    1]]),\n",
       " 'clean_answer': 1370,\n",
       " 'patch_prefix': tensor([[ 310,  401,  114, 7434,   45,    5,    1]]),\n",
       " 'patch_answer': 404,\n",
       " 'UID': 'anaphor_gender_agreement',\n",
       " 'linguistics_term': 'anaphor_agreement',\n",
       " 'prefix_length_wo_pad': 7}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blimp_examples = load_blimp_examples(tokenizer, pad_to_length=32, n_samples=10)\n",
    "blimp_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded huggingface DS\n",
      "loaded local DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 255.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,   27,   44, 4045,   23,  463,   17,    1]]),\n",
       " 'clean_answer': 49,\n",
       " 'distractors': [3895, 1224, 121, 1017, 303, 55, 175],\n",
       " 'question_type': 'is this',\n",
       " 'prefix_length_wo_pad': 7,\n",
       " 'pixel_values': tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1008, -2.1179,  ..., -2.1008, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1008, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1008, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0182, -2.0357,  ..., -2.0182, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0182, -2.0357, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0182, -2.0357, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.7870, -1.7870,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.7870, -1.8044, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.7870, -1.8044, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.7870]]]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and prepare data\n",
    "vqa_examples = load_vqa_examples(tokenizer, img_processor, pad_to_length=32, n_samples=10)\n",
    "vqa_examples[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mean_activations(examples, model, submodules, batch_size, num_examples, noimg=False):\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "    device = \"cuda\"\n",
    "    num_examples = min([num_examples, len(examples)])\n",
    "    batches = [\n",
    "        examples[i:min(i + batch_size,num_examples)] for i in range(0, num_examples, batch_size)\n",
    "    ]\n",
    "\n",
    "    # Initialize storage for cumulative activations and count of samples\n",
    "    cumulative_activations = {submodule: 0 for submodule in submodules}\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in tqdm(batches):\n",
    "        clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "    \n",
    "        # clean run -> model can be approximated through linear function of its activations\n",
    "        hidden_states_clean = {}\n",
    "        if noimg:\n",
    "            with model.trace(clean_inputs, **tracer_kwargs), t.no_grad():\n",
    "                for submodule in submodules:\n",
    "                    x = submodule.output\n",
    "                    hidden_states_clean[submodule] = x.save()\n",
    "        else:\n",
    "            img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "            with model.trace(clean_inputs, pixel_values=img_inputs, **tracer_kwargs), t.no_grad():\n",
    "                for submodule in submodules:\n",
    "                    x = submodule.output\n",
    "                    hidden_states_clean[submodule] = x.save()\n",
    "        hidden_states_clean = {k : v.value for k, v in hidden_states_clean.items()}\n",
    "\n",
    "        batch_size = next(iter(hidden_states_clean.values())).shape[0]  # Assuming shape [batch_size, ...]\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # Sum across the batch (dim=0)\n",
    "        for submodule, state in hidden_states_clean.items():\n",
    "            cumulative_activations[submodule] += state.sum(dim=(0, 1))  \n",
    "        \n",
    "        hidden_states_clean = None\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Compute mean activation by dividing the cumulative activations by the total number of samples\n",
    "    mean_activations = {submodule: cum_act / total_samples for submodule, cum_act in cumulative_activations.items()}\n",
    "\n",
    "    return mean_activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution patching with integrated gradients\n",
    "def _pe_ig(\n",
    "        clean,\n",
    "        img_inputs,\n",
    "        model,\n",
    "        submodules,\n",
    "        hidden_states_mean,\n",
    "        metric_fn,\n",
    "        steps=10,\n",
    "        metric_kwargs=dict()):\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "    \n",
    "    # clean run -> model can be approximated through linear function of its activations\n",
    "    hidden_states_clean = {}\n",
    "    if img_inputs is None:\n",
    "        with model.trace(clean, **tracer_kwargs), t.no_grad():\n",
    "            for submodule in submodules:\n",
    "                x = submodule.output\n",
    "                hidden_states_clean[submodule] = x.save()\n",
    "            metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "    else:\n",
    "        with model.trace(clean, pixel_values=img_inputs, **tracer_kwargs), t.no_grad(): \n",
    "            for submodule in submodules:\n",
    "                x = submodule.output\n",
    "                hidden_states_clean[submodule] = x.save()\n",
    "            metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "    hidden_states_clean = {k : v.value for k, v in hidden_states_clean.items()}\n",
    "\n",
    "\n",
    "    effects = {}\n",
    "    deltas = {}\n",
    "    grads = {}\n",
    "    for submodule in submodules:\n",
    "        clean_state = hidden_states_clean[submodule]\n",
    "        mean_state = hidden_states_mean[submodule]\n",
    "        with model.trace(**tracer_kwargs) as tracer:\n",
    "            metrics = []\n",
    "            fs = []\n",
    "            for step in range(steps):\n",
    "                alpha = step / steps\n",
    "                f = (1 - alpha) * clean_state + alpha * mean_state\n",
    "                f.retain_grad()\n",
    "                fs.append(f)\n",
    "                if img_inputs is None:\n",
    "                    with tracer.invoke(clean, scan=tracer_kwargs['scan']):\n",
    "                        submodule.output = f\n",
    "                        metrics.append(metric_fn(model, **metric_kwargs))\n",
    "                else:\n",
    "                    with tracer.invoke(clean, pixel_values=img_inputs, scan=tracer_kwargs['scan']):\n",
    "                        submodule.output = f\n",
    "                        metrics.append(metric_fn(model, **metric_kwargs))\n",
    "            metric = sum([m for m in metrics])\n",
    "            metric.sum().backward(retain_graph=True) # TODO : why is this necessary? Probably shouldn't be, contact jaden\n",
    "        \n",
    "        mean_grad = sum([f.grad for f in fs]) / steps\n",
    "        # mean_residual_grad = sum([f.grad for f in fs]) / steps\n",
    "        grad = mean_grad\n",
    "        delta = (mean_state - clean_state).detach() if mean_state is not None else -clean_state.detach()\n",
    "        effect = t.mul(grad, delta)\n",
    "\n",
    "        effects[submodule] = effect\n",
    "        deltas[submodule] = delta\n",
    "        grads[submodule] = grad\n",
    "    \n",
    "    return (effects, deltas, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,   51,   54,  137,  951,   10,  189, 1183,\n",
       "             51,   54,  137, 1183,  189,  951,   10,    1]]),\n",
       "  'correct_idx': [0, 1, 2, 3, 4, 5, 6],\n",
       "  'incorrect_idx': [7, 8, 9, 10, 11, 12, 13],\n",
       "  'tag': 'Noun',\n",
       "  'collapsed_tag': 'Object',\n",
       "  'pixel_values': tensor([[[[-1.4843, -1.4843, -1.4500,  ..., -0.3883, -0.3883, -0.1999],\n",
       "            [-1.4843, -1.4672, -1.4329,  ..., -0.4226, -0.4226, -0.2684],\n",
       "            [-1.4843, -1.4672, -1.4329,  ..., -0.4397, -0.4054, -0.3369],\n",
       "            ...,\n",
       "            [-1.2617, -1.0733, -1.1418,  ..., -1.2274, -1.3644, -1.4843],\n",
       "            [-1.2103, -0.9705, -1.2445,  ..., -1.3644, -1.4672, -1.5870],\n",
       "            [-1.1589, -1.2788, -1.2959,  ..., -1.3987, -1.4158, -1.5014]],\n",
       "  \n",
       "           [[-1.0553, -1.0378, -1.0028,  ...,  0.3102,  0.2752,  0.4153],\n",
       "            [-1.0553, -1.0378, -0.9853,  ...,  0.2752,  0.2577,  0.3627],\n",
       "            [-1.0553, -1.0203, -0.9853,  ...,  0.2577,  0.2752,  0.3277],\n",
       "            ...,\n",
       "            [-1.4055, -1.3179, -1.3704,  ..., -1.3529, -1.4755, -1.5630],\n",
       "            [-1.4055, -1.2479, -1.4405,  ..., -1.4755, -1.5455, -1.5980],\n",
       "            [-1.3529, -1.4755, -1.4580,  ..., -1.4930, -1.5105, -1.5455]],\n",
       "  \n",
       "           [[-0.4101, -0.3927, -0.3404,  ...,  1.1411,  1.1062,  1.1585],\n",
       "            [-0.4101, -0.3753, -0.3230,  ...,  1.1237,  1.1237,  1.1411],\n",
       "            [-0.3927, -0.3753, -0.3230,  ...,  1.1062,  1.0888,  1.1062],\n",
       "            ...,\n",
       "            [-1.4036, -1.3513, -1.4210,  ..., -1.3513, -1.4036, -1.4559],\n",
       "            [-1.3687, -1.3164, -1.4384,  ..., -1.4036, -1.4384, -1.4559],\n",
       "            [-1.3861, -1.4210, -1.4559,  ..., -1.4036, -1.4036, -1.4036]]]]),\n",
       "  'prefix_length_wo_pad': 15},\n",
       " {'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    9,\n",
       "            508,   27,  225,  266,    9,  485,   27, 1243,  794,    9,  508,   27,\n",
       "           1243,  794,  266,    9,  485,   27,  225,    1]]),\n",
       "  'correct_idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "  'incorrect_idx': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "  'tag': 'Adjective-Speed',\n",
       "  'collapsed_tag': 'Relation',\n",
       "  'pixel_values': tensor([[[[-0.1314, -0.1143, -0.0801,  ..., -1.2103, -0.7479, -0.3027],\n",
       "            [-0.1143, -0.1143, -0.1143,  ..., -0.9192, -0.3883, -0.1999],\n",
       "            [-0.1657, -0.1486, -0.1314,  ..., -0.4397, -0.2342, -0.2684],\n",
       "            ...,\n",
       "            [-1.0390, -1.0562, -1.0904,  ..., -1.6213, -1.6213, -1.6042],\n",
       "            [-1.2103, -1.1760, -1.1589,  ..., -1.6213, -1.6042, -1.6042],\n",
       "            [-1.3302, -1.2445, -1.2274,  ..., -1.6042, -1.5870, -1.5870]],\n",
       "  \n",
       "           [[-1.4405, -1.4405, -1.4230,  ..., -1.6681, -1.4755, -1.3004],\n",
       "            [-1.4405, -1.4405, -1.4405,  ..., -1.5455, -1.3004, -1.2129],\n",
       "            [-1.4580, -1.4580, -1.4405,  ..., -1.3354, -1.2304, -1.2479],\n",
       "            ...,\n",
       "            [-1.7381, -1.7381, -1.7556,  ..., -1.8782, -1.8957, -1.8782],\n",
       "            [-1.7731, -1.7731, -1.7906,  ..., -1.8782, -1.8782, -1.8782],\n",
       "            [-1.8081, -1.7906, -1.8081,  ..., -1.8431, -1.8606, -1.8431]],\n",
       "  \n",
       "           [[-1.7870, -1.7870, -1.8044,  ..., -1.6476, -1.5953, -1.5430],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.6127, -1.5430, -1.4733],\n",
       "            [-1.7870, -1.7696, -1.7696,  ..., -1.5604, -1.4733, -1.4559],\n",
       "            ...,\n",
       "            [-1.7696, -1.7696, -1.7870,  ..., -1.7522, -1.7522, -1.7347],\n",
       "            [-1.7696, -1.7696, -1.7696,  ..., -1.7347, -1.7347, -1.7347],\n",
       "            [-1.7347, -1.7347, -1.7522,  ..., -1.7173, -1.7347, -1.7347]]]]),\n",
       "  'prefix_length_wo_pad': 21}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [winoground_examples[i:min(i + 2, 10)] for i in range(0, 10, 2)][0]\n",
    "clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_idxs = [[0,1,2], [4,5,6,7]]\n",
    "incorrect_idxs = [[1,2], [5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7798,  0.7615], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([-1.1319, -3.8153], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.3521, 4.5768], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(-1 )  # [1, seq]\n",
    "                answer = float(logits.sum())\n",
    "\"\"\"\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "with model.trace(**tracer_kwargs) as tracer:\n",
    "            \n",
    "    with tracer.invoke(clean_inputs, scan=tracer_kwargs['scan']):\n",
    "        outputs = model.output.output.save()\n",
    "\n",
    "\n",
    "a = t.gather(outputs[:,-1,:], dim=-1, index=t.tensor([17, 18]).to(\"cuda\").view(-1, 1)).squeeze(-1)\n",
    "b = t.gather(outputs[:,-1,:], dim=-1, index=t.tensor([19, 13]).to(\"cuda\").view(-1, 1)).squeeze(-1)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a-b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.0794, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "torch.Size([1])\n",
      "tensor(14.7316, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([0., 0.], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "correct_sent_logits = []\n",
    "incorrect_sent_logits = []\n",
    "for i, idx in enumerate(correct_idxs):\n",
    "    use = t.tensor([idx]).to(\"cuda\")\n",
    "    logits = torch.gather(outputs[i,:,:], dim=1, index=use).squeeze(-1) # [1, seq]\n",
    "    print(logits.sum())\n",
    "    print(logits.sum().unsqueeze(0).shape)\n",
    "    correct_sent_logits.append(logits.sum().unsqueeze(0))\n",
    "correct_sent_logits = torch.cat(correct_sent_logits, dim=0)\n",
    "print(correct_sent_logits-correct_sent_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_neurons(examples, batch_size, mlps, mean_activations, task):\n",
    "    # uses attribution patching to identify most important neurons for subtask\n",
    "    num_examples = len(examples)\n",
    "    batches = [examples[i:min(i + batch_size, num_examples)] for i in range(0, num_examples, batch_size)]\n",
    "    device = \"cuda\"\n",
    "\n",
    "    sum_effects = {}\n",
    "\n",
    "    for batch in tqdm(batches): \n",
    "        clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "\n",
    "        if task == \"vqa\":\n",
    "            clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "            img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "\n",
    "            first_distractor_idxs = t.tensor([e['distractors'][0] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "            def metric(model):\n",
    "                # compute difference between correct answer and first distractor\n",
    "                # TODO: compute avg difference between correct answer and each distractor\n",
    "                #embds_out = model.output.output.save()\n",
    "                \n",
    "                return (\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=first_distractor_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "                )\n",
    "        \n",
    "        elif task == \"blimp\":\n",
    "            clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "            img_inputs = None\n",
    "            \n",
    "            patch_answer_idxs = t.tensor([e['patch_answer'] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "            def metric(model):\n",
    "                \n",
    "                return (\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "                )\n",
    "            \n",
    "        elif task == \"winoground\":\n",
    "            correct_idxs = [e['correct_idx'] for e in batch]\n",
    "            incorrect_idxs = [e['incorrect_idx'] for e in batch]\n",
    "            img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "\n",
    "\n",
    "            def metric(model):\n",
    "                correct_sent_logits = []\n",
    "                incorrect_sent_logits = []\n",
    "\n",
    "                outputs = model.output.output\n",
    "                \n",
    "                for i, idx in enumerate(correct_idxs):\n",
    "                    correct_pos = t.tensor([idx]).to(\"cuda\")\n",
    "                    logits = torch.gather(outputs[i,:,:], dim=1, index=correct_pos).squeeze(-1) # [1, seq]\n",
    "                    correct_sent_logits.append(logits.sum().unsqueeze(0))\n",
    "                correct_sent_logits = torch.cat(correct_sent_logits, dim=0)\n",
    "                \n",
    "                for i, idx in enumerate(incorrect_idxs):\n",
    "                    incorrect_pos = t.tensor([idx]).to(\"cuda\")\n",
    "                    logits = torch.gather(outputs[i,:,:], dim=1, index=incorrect_pos).squeeze(-1) # [1, seq]\n",
    "                    incorrect_sent_logits.append(logits.sum().unsqueeze(0))\n",
    "                incorrect_sent_logits = torch.cat(incorrect_sent_logits, dim=0)\n",
    "\n",
    "                result = incorrect_sent_logits - correct_sent_logits\n",
    "                \n",
    "                return result\n",
    "\n",
    "        effects, _, _ = _pe_ig(\n",
    "            clean_inputs,\n",
    "            img_inputs,\n",
    "            model,\n",
    "            mlps,\n",
    "            mean_activations,\n",
    "            metric,\n",
    "            steps=10,\n",
    "            metric_kwargs=dict())\n",
    "        \n",
    "        \n",
    "        for submodule in mlps:\n",
    "            if submodule not in sum_effects:\n",
    "                sum_effects[submodule] = effects[submodule].sum(dim=1).sum(dim=0)\n",
    "            else:\n",
    "                sum_effects[submodule] += effects[submodule].sum(dim=1).sum(dim=0)\n",
    "\n",
    "    # Print top 1% neurons in each submodule (ndim=3072)\n",
    "    k = 31\n",
    "\n",
    "    top_neurons = {}\n",
    "    for idx, submodule in enumerate(mlps):\n",
    "        sum_effects[submodule] /= num_examples\n",
    "        v, i = t.topk(sum_effects[submodule].flatten(), k)  # v=top effects, i=top indices\n",
    "        top_neurons[f\"mlp_{idx}\"] = (i,v)\n",
    "    return top_neurons\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.01it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "#### VQA\n",
    "####\n",
    "\n",
    "batch_size = 2  #16\n",
    "num_examples = 8  #-1\n",
    "\n",
    "mlps = [submodules[submodule] for submodule in submodules if submodule.startswith(\"mlp\")]\n",
    "mlps = mlps[:2]\n",
    "\n",
    "# precompute mean activations on VQA\n",
    "mean_activations_vqa = compute_mean_activations(vqa_examples, model, mlps, batch_size, num_examples)\n",
    "\n",
    "# extract most important neurons per subtask of VQA by using attribution patching between clean and mean activations\n",
    "subtasks = {}\n",
    "for e in vqa_examples:\n",
    "    subtask = e[\"question_type\"]\n",
    "    if subtask in subtasks:\n",
    "        subtasks[subtask].append(e)\n",
    "    else:\n",
    "        subtasks[subtask] = [e]\n",
    "\n",
    "# for each subtask, compute top neurons and save\n",
    "subtasks_neurons = {}\n",
    "for subtask, examples in subtasks.items():\n",
    "    if len(subtasks_neurons) == 2:\n",
    "        break\n",
    "    top_neurons = get_important_neurons(examples, batch_size, mlps, mean_activations_vqa, task=\"vqa\")\n",
    "    subtasks_neurons[subtask] = top_neurons\n",
    "\n",
    "\n",
    "with open(f\"data/vqa_top_neurons_per_subtask.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subtasks_neurons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/vqa_top_neurons_per_subtask.pkl\", \"rb\") as g:\n",
    "    subtasks_neurons = pickle.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['is this', 'what is the'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtasks_neurons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.00it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n",
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:02,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n",
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:01<00:01,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n",
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:01,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n",
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:02<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n",
      "metric: InterventionProxy (add_9): <class 'inspect._empty'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "#### BLIMP\n",
    "####\n",
    "\n",
    "batch_size = 2  #16\n",
    "num_examples = 8  #-1\n",
    "\n",
    "mlps = [submodules[submodule] for submodule in submodules if submodule.startswith(\"mlp\")]\n",
    "mlps = mlps[:2]\n",
    "\n",
    "# precompute mean activations on BLIMP\n",
    "mean_activations_blimp = compute_mean_activations(blimp_examples, model, mlps, batch_size, num_examples, noimg=True)\n",
    "\n",
    "# extract most important neurons per subtask of VQA by using attribution patching between clean and mean activations\n",
    "subtasks = {}\n",
    "for e in blimp_examples:\n",
    "    subtask = e[\"UID\"]\n",
    "    if subtask in subtasks:\n",
    "        subtasks[subtask].append(e)\n",
    "    else:\n",
    "        subtasks[subtask] = [e]\n",
    "\n",
    "# for each subtask, compute top neurons and save\n",
    "subtasks_neurons = {}\n",
    "for subtask, examples in subtasks.items():\n",
    "    if len(subtasks_neurons) == 2:\n",
    "        break\n",
    "    top_neurons = get_important_neurons(examples, batch_size, mlps, mean_activations_blimp, task=\"blimp\")\n",
    "    subtasks_neurons[subtask] = top_neurons\n",
    "\n",
    "\n",
    "with open(f\"data/blimp_top_neurons_per_subtask.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subtasks_neurons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.18it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 32 but got size 229 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subtasks_neurons) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     top_neurons \u001b[38;5;241m=\u001b[39m \u001b[43mget_important_neurons\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_activations_winoground\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwinoground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     subtasks_neurons[subtask] \u001b[38;5;241m=\u001b[39m top_neurons\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/winoground_top_neurons_per_subtask.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[7], line 69\u001b[0m, in \u001b[0;36mget_important_neurons\u001b[0;34m(examples, batch_size, mlps, mean_activations, task)\u001b[0m\n\u001b[1;32m     65\u001b[0m         result \u001b[38;5;241m=\u001b[39m incorrect_sent_logits \u001b[38;5;241m-\u001b[39m correct_sent_logits\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m---> 69\u001b[0m effects, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_pe_ig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmean_activations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m mlps:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m submodule \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sum_effects:\n",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m, in \u001b[0;36m_pe_ig\u001b[0;34m(clean, img_inputs, model, submodules, hidden_states_mean, metric_fn, steps, metric_kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 metrics\u001b[38;5;241m.\u001b[39mappend(metric_fn(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetric_kwargs))\n\u001b[1;32m     52\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics])\n\u001b[0;32m---> 53\u001b[0m     metric\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# TODO : why is this necessary? Probably shouldn't be, contact jaden\u001b[39;00m\n\u001b[1;32m     55\u001b[0m mean_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([f\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs]) \u001b[38;5;241m/\u001b[39m steps\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# mean_residual_grad = sum([f.grad for f in fs]) / steps\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/contexts/Runner.py:49\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/contexts/Tracer.py:71\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batched_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo input was provided to the tracing context.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mtracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:253\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m     ),\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    255\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:452\u001b[0m, in \u001b[0;36mHookHandler.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    449\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:253\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m     ),\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:356\u001b[0m, in \u001b[0;36mNNsight._execute\u001b[0;34m(self, *prepared_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepared_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/alina/Documents/phd/Code/babylm_GIT/models2/base_git_1vd125_s1/epoch17/modeling_git.py:62\u001b[0m, in \u001b[0;36mGitForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     77\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:1275\u001b[0m, in \u001b[0;36mGitModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1273\u001b[0m         combined_attention_mask[:, :, \u001b[38;5;241m-\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m] :, \u001b[38;5;241m-\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m] :] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m expanded_attn_mask\n\u001b[0;32m-> 1275\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values_present\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:452\u001b[0m, in \u001b[0;36mGitEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, pixel_values_present, return_dict)\u001b[0m\n\u001b[1;32m    443\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    444\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    445\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m         output_attentions,\n\u001b[1;32m    450\u001b[0m     )\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values_present\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:388\u001b[0m, in \u001b[0;36mGitLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, past_key_value, output_attentions, pixel_values_present)\u001b[0m\n\u001b[1;32m    385\u001b[0m outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    386\u001b[0m present_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 388\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/git/modeling_git.py:399\u001b[0m, in \u001b[0;36mGitLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 399\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/torch/nn/modules/module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:439\u001b[0m, in \u001b[0;36mHookHandler.__enter__.<locals>.output_hook\u001b[0;34m(module, input, output, module_path)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutput_hook\u001b[39m(module, \u001b[38;5;28minput\u001b[39m, output, module_path\u001b[38;5;241m=\u001b[39mmodule_path):\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:249\u001b[0m, in \u001b[0;36mNNsight.interleave.<locals>.<lambda>\u001b[0;34m(activations, module_path)\u001b[0m\n\u001b[1;32m    239\u001b[0m inputs, total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    241\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m    246\u001b[0m     input_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m activations, module_path: intervene(\n\u001b[1;32m    247\u001b[0m         activations, module_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, intervention_handler\n\u001b[1;32m    248\u001b[0m     ),\n\u001b[0;32m--> 249\u001b[0m     output_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m activations, module_path: \u001b[43mintervene\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_handler\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    252\u001b[0m ):\n\u001b[1;32m    253\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    255\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:362\u001b[0m, in \u001b[0;36mintervene\u001b[0;34m(activations, module_path, key, intervention_handler)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# If we narrowed any data, we need to concat it with data before and after it.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m narrowed:\n\u001b[0;32m--> 362\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintervention_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Otherwise just return the whole value as the activations.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     activations \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:272\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(activations, value, batch_start, batch_size, total_batch_size)\u001b[0m\n\u001b[1;32m    264\u001b[0m post \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    265\u001b[0m     activations,\n\u001b[1;32m    266\u001b[0m     narrow2,\n\u001b[1;32m    267\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    268\u001b[0m )\n\u001b[1;32m    270\u001b[0m orig_sizes \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mapply(activations, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sizes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/intervention.py:227\u001b[0m, in \u001b[0;36mconcat.<locals>._concat\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    225\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_size \u001b[38;5;241m==\u001b[39m orig_size:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 32 but got size 229 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "####\n",
    "#### Winoground\n",
    "####\n",
    "\n",
    "batch_size = 2  #16\n",
    "num_examples = 8  #-1\n",
    "\n",
    "mlps = [submodules[submodule] for submodule in submodules if submodule.startswith(\"mlp\")]\n",
    "mlps = mlps[:2]\n",
    "\n",
    "# precompute mean activations on BLIMP\n",
    "mean_activations_winoground = compute_mean_activations(winoground_examples, model, mlps, batch_size, num_examples, noimg=False)\n",
    "\n",
    "# extract most important neurons per subtask of VQA by using attribution patching between clean and mean activations\n",
    "subtasks = {}\n",
    "for e in winoground_examples:\n",
    "    subtask = e[\"tag\"]\n",
    "    if subtask in subtasks:\n",
    "        subtasks[subtask].append(e)\n",
    "    else:\n",
    "        subtasks[subtask] = [e]\n",
    "\n",
    "# for each subtask, compute top neurons and save\n",
    "subtasks_neurons = {}\n",
    "for subtask, examples in subtasks.items():\n",
    "    if len(subtasks_neurons) == 2:\n",
    "        break\n",
    "    top_neurons = get_important_neurons(examples, batch_size, mlps, mean_activations_winoground, task=\"winoground\")\n",
    "    subtasks_neurons[subtask] = top_neurons\n",
    "\n",
    "\n",
    "with open(f\"data/winoground_top_neurons_per_subtask.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subtasks_neurons, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
