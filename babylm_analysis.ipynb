{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "import torch as t\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from loading_utils import load_vqa_examples, load_blimp_examples, load_winoground_examples\n",
    "\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from nnsight import NNsight\n",
    "import importlib.util\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from babylm_analysis import _pe_ig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(own_model=True):\n",
    "    if own_model:\n",
    "        model_path = \"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/\"\n",
    "        spec = importlib.util.spec_from_file_location(\"GitForCausalLM\", f\"{model_path}modeling_git.py\")\n",
    "        git_module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[\"git_module\"] = git_module\n",
    "        spec.loader.exec_module(git_module)\n",
    "        GitForCausalLM = git_module.GitForCausalLM\n",
    "\n",
    "        model = GitForCausalLM.from_pretrained(model_path) \n",
    "        ckpt = torch.load(model_path + \"pytorch_model.bin\") # TODO: newly initialized for vision encoder: ['pooler.dense.bias', 'pooler.dense.weight']\n",
    "        model.load_state_dict(ckpt, strict=False)  \n",
    "        \n",
    "    else:\n",
    "        model_path = \"babylm/git-2024\"\n",
    "\n",
    "        from transformers import GitForCausalLM as OGModel\n",
    "\n",
    "        model = OGModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "    # load tokenizer and img processor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    img_processor = AutoProcessor.from_pretrained(model_path,trust_remote_code=True)\n",
    "    \n",
    "    nnsight_model = NNsight(model, device_map=\"cuda\")\n",
    "    nnsight_model.to(\"cuda\")\n",
    "\n",
    "    return nnsight_model, tokenizer, img_processor\n",
    "\n",
    "\n",
    "def extract_submodules(model):\n",
    "    submodules = {}\n",
    "    for idx, layer in enumerate(model.git.encoder.layer):\n",
    "        submodules[f\"mlp.{idx}\"] = layer.intermediate    # output of MLP\n",
    "        submodules[f\"attn.{idx}\"] = layer.attention  # output of attention\n",
    "        submodules[f\"resid.{idx}\"] = layer      # output of whole layer\n",
    "    return submodules\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitAttention'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.git.modeling_git.GitLayer'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.vit.modeling_vit.ViTSdpaAttention'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'transformers.models.vit.modeling_vit.ViTLayer'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/nnsight/envoy.py:93: UserWarning: Module of type `<class 'GitForCausalLM.GitForCausalLM'>` has pre-defined a `output` attribute. nnsight access for `output` will be mounted at `.nns_output` instead of `.output` for this module only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load and prepare model\n",
    "model, tokenizer, img_processor = load_model(own_model=True)\n",
    "submodules = extract_submodules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for facebook/winoground contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/facebook/winoground\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded huggingface DS\n",
      "loaded local DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 24.35it/s]\n"
     ]
    }
   ],
   "source": [
    "winoground_examples = load_winoground_examples(tokenizer, img_processor, pad_to_length=32, n_samples=10, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,  310,  401,  114, 7434,   45,    5,    1]]),\n",
       " 'clean_answer': 1370,\n",
       " 'patch_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,  310,  401,  114, 7434,   45,    5,    1]]),\n",
       " 'patch_answer': 404,\n",
       " 'UID': 'anaphor_gender_agreement',\n",
       " 'linguistics_term': 'anaphor_agreement',\n",
       " 'prefix_length_wo_pad': 7}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blimp_examples = load_blimp_examples(tokenizer, pad_to_length=32, n_samples=10, local=True)\n",
    "blimp_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded huggingface DS\n",
      "loaded local DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 98.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clean_prefix': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,   27,   44, 4045,   23,  463,   17,    1]]),\n",
       " 'clean_answer': 49,\n",
       " 'distractors': [3895, 1224, 121, 1017, 303, 55, 175],\n",
       " 'question_type': 'is this',\n",
       " 'prefix_length_wo_pad': 7,\n",
       " 'pixel_values': tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1008, -2.1179,  ..., -2.1008, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1008, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1008, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0182, -2.0357,  ..., -2.0182, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0182, -2.0357, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0182, -2.0357, -2.0182,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.7870, -1.7870,  ..., -1.7870, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.7870, -1.8044, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.7870, -1.8044, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.7870]]]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and prepare data\n",
    "vqa_examples = load_vqa_examples(tokenizer, img_processor, pad_to_length=32, n_samples=10, local=True)\n",
    "vqa_examples[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mean_activations(examples, model, submodules, batch_size, noimg=False, file_prefix=None):\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "    device = \"cuda\"\n",
    "    num_examples = len(examples)\n",
    "    batches = [\n",
    "        examples[i:min(i + batch_size,num_examples)] for i in range(0, num_examples, batch_size)\n",
    "    ]\n",
    "\n",
    "    def extract_hidden_states(submodule):\n",
    "        total_samples = 0\n",
    "        # Initialize storage for cumulative activations and count of samples\n",
    "        cumulative_activations = 0\n",
    "\n",
    "        for batch in tqdm(batches):\n",
    "            clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "        \n",
    "            # clean run -> model can be approximated through linear function of its activations\n",
    "            hidden_states_clean = {}\n",
    "            #with autocast():\n",
    "            if noimg:\n",
    "                with model.trace(clean_inputs, **tracer_kwargs), t.no_grad():\n",
    "                    x = submodule.output\n",
    "                    hidden_states_clean = x.save()\n",
    "            else:\n",
    "                img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "                with model.trace(clean_inputs, pixel_values=img_inputs, **tracer_kwargs), t.no_grad():\n",
    "                    x = submodule.output\n",
    "                    hidden_states_clean = x.save()\n",
    "            hidden_states_clean = hidden_states_clean.value\n",
    "\n",
    "            batch_size = clean_inputs.shape[0]  # Assuming shape [batch_size, ...]\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # Sum across the batch (dim=0)\n",
    "            cumulative_activations += hidden_states_clean.sum(dim=(0, 1)).detach().cpu()  # detach\n",
    "            \n",
    "            hidden_states_clean = None\n",
    "            clean_inputs = None\n",
    "            state = None\n",
    "            x = None\n",
    "            batch_size = None\n",
    "            del hidden_states_clean, clean_inputs, state, x, batch_size\n",
    "            if not noimg:\n",
    "                img_inputs = None\n",
    "                del img_inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Compute mean activation by dividing the cumulative activations by the total number of samples\n",
    "        mean_activations = cumulative_activations / total_samples\n",
    "\n",
    "        cumulative_activations = None\n",
    "        del cumulative_activations\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return mean_activations\n",
    "\n",
    "    mean_act_files = []\n",
    "    for i, submodule in enumerate(submodules):\n",
    "        submodule_acts = extract_hidden_states(submodule)\n",
    "        filename = f\"mean_activations/{file_prefix}_mean_acts_{i}.npy\"\n",
    "        np.save(filename, submodule_acts)\n",
    "\n",
    "        submodule_acts = None\n",
    "        del submodule_acts\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        mean_act_files.append(filename)\n",
    "\n",
    "    return mean_act_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_neurons(examples, batch_size, mlps, pad_len, mean_act_files, task, noimg):\n",
    "    # uses attribution patching to identify most important neurons for subtask\n",
    "    num_examples = len(examples)\n",
    "    batches = [examples[i:min(i + batch_size, num_examples)] for i in range(0, num_examples, batch_size)]\n",
    "    device = \"cuda\"\n",
    "\n",
    "    sum_effects = {}\n",
    "\n",
    "    for batch in tqdm(batches):\n",
    "        \n",
    "        clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "\n",
    "        if task == \"vqa\":\n",
    "            clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "            if noimg:\n",
    "                img_inputs = None\n",
    "            else:\n",
    "                img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "\n",
    "            first_distractor_idxs = t.tensor([e['distractors'][0] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "            def metric(model):\n",
    "                # compute difference between correct answer and first distractor\n",
    "                # TODO: compute avg difference between correct answer and each distractor\n",
    "                #embds_out = model.output.output.save()\n",
    "                \n",
    "                return (\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=first_distractor_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "                )\n",
    "            \n",
    "            effects, _, _ = _pe_ig(\n",
    "                clean_inputs,\n",
    "                img_inputs,\n",
    "                model,\n",
    "                mlps,\n",
    "                mean_act_files,\n",
    "                metric,\n",
    "                pad_len,\n",
    "                steps=10,\n",
    "                metric_kwargs=dict())\n",
    "        \n",
    "        elif task == \"blimp\":\n",
    "            img_inputs = None\n",
    "            clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "            patch_answer_idxs = t.tensor([e['patch_answer'] for e in batch], dtype=t.long, device=device)\n",
    "\n",
    "            def metric(model):\n",
    "                \n",
    "                return (\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "                    t.gather(model.output.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "                )\n",
    "        \n",
    "\n",
    "            effects, _, _ = _pe_ig(\n",
    "                clean_inputs,\n",
    "                img_inputs,\n",
    "                model,\n",
    "                mlps,\n",
    "                mean_act_files,\n",
    "                metric,\n",
    "                pad_len,\n",
    "                steps=10,\n",
    "                metric_kwargs=dict())\n",
    "            \n",
    "        elif task == \"winoground\":\n",
    "            if noimg:\n",
    "                img_inputs = None\n",
    "            else:\n",
    "                img_inputs = t.cat([e['pixel_values'] for e in batch], dim=0).to(device)\n",
    "\n",
    "            correct_idxs = [e[\"correct_idx\"] for e in batch]\n",
    "            incorrect_idxs = [e[\"incorrect_idx\"] for e in batch]\n",
    "\n",
    "            def metric(model):\n",
    "                correct_sent_logits = []\n",
    "                incorrect_sent_logits = []\n",
    "                for i, (idx, cf_idx) in enumerate(zip(correct_idxs, incorrect_idxs)):\n",
    "                    logits = torch.gather(model.output.output[i,:,:], dim=1, index=t.tensor([idx]).to(\"cuda\")).squeeze(-1) # [1, seq]\n",
    "                    cf_logits = torch.gather(model.output.output[i,:,:], dim=1, index=t.tensor([cf_idx]).to(\"cuda\")).squeeze(-1) # [1, seq]\n",
    "                    correct_sent_logits.append(logits.sum().unsqueeze(0))\n",
    "                    incorrect_sent_logits.append(cf_logits.sum().unsqueeze(0))\n",
    "                correct_sent_logits = torch.cat(correct_sent_logits, dim=0)\n",
    "                incorrect_sent_logits = torch.cat(incorrect_sent_logits, dim=0)\n",
    "                return incorrect_sent_logits-correct_sent_logits\n",
    "\n",
    "\n",
    "            effects, _, _ = _pe_ig(\n",
    "                clean_inputs,\n",
    "                img_inputs,\n",
    "                model,\n",
    "                mlps,\n",
    "                mean_act_files,\n",
    "                metric,\n",
    "                pad_len,\n",
    "                steps=10,\n",
    "                metric_kwargs=dict())\n",
    "        \n",
    "        \n",
    "        for submodule in mlps:\n",
    "            if submodule not in sum_effects:\n",
    "                sum_effects[submodule] = effects[submodule].sum(dim=1).sum(dim=0)\n",
    "            else:\n",
    "                sum_effects[submodule] += effects[submodule].sum(dim=1).sum(dim=0)\n",
    "\n",
    "    # Print top 100 neurons in each submodule (ndim=3072)\n",
    "    k = 100\n",
    "\n",
    "    top_neurons = {}\n",
    "    for idx, submodule in enumerate(mlps):\n",
    "        sum_effects[submodule] /= num_examples\n",
    "        v, i = t.topk(sum_effects[submodule].flatten(), k)  # v=top effects, i=top indices\n",
    "        top_neurons[f\"mlp_{idx}\"] = (i.cpu(),v.cpu())\n",
    "    return top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  #16\n",
    "num_examples = 8  #-1\n",
    "task = \"vqa\"\n",
    "model_name = \"git_1vd125_s1\"\n",
    "epoch = 23\n",
    "local = True\n",
    "pad_len =32\n",
    "\n",
    "def run_task(task):\n",
    "    mlps = [submodules[submodule] for submodule in submodules if submodule.startswith(\"mlp\")]\n",
    "    mlps = mlps[:2]\n",
    "\n",
    "    noimg = False\n",
    "\n",
    "    # load and prepare data\n",
    "    if task == \"vqa\":\n",
    "        examples = load_vqa_examples(tokenizer, img_processor, pad_to_length=32, n_samples=num_examples, local=local)\n",
    "        subtask_key = \"question_type\"\n",
    "    elif task == \"blimp\":\n",
    "        noimg = True\n",
    "        examples = load_blimp_examples(tokenizer, pad_to_length=32, n_samples=num_examples, local=local)\n",
    "        subtask_key = \"linguistics_term\"\n",
    "        mean_act_files = None\n",
    "    elif task == \"winoground\":\n",
    "        examples = load_winoground_examples(tokenizer, img_processor, pad_to_length=32, n_samples=num_examples, local=local)\n",
    "        subtask_key = \"secondary_tag\"\n",
    "    else:\n",
    "        print(f\"{task} is not implemented\")\n",
    "    print(\"loaded samples\")\n",
    "\n",
    "    \n",
    "    prefix = f\"{task}_{model_name}_e{epoch}_n{num_examples if num_examples != -1 else 'all'}{'_noimg' if noimg else ''}\"\n",
    "    mean_act_files = []\n",
    "    for file in os.listdir(\"mean_activations/\"):\n",
    "        if file.startswith(prefix+\"_mean_acts\"):\n",
    "            mean_act_files.append(f\"mean_activations/{file}\")\n",
    "    if len(mean_act_files) != len(mlps):\n",
    "        mean_act_files = compute_mean_activations(examples, model, mlps, batch_size=128, noimg=noimg, file_prefix=prefix)\n",
    "        print(f\"computed mean activations\")\n",
    "    else:\n",
    "        print(\"retrieved precomputed mean activations\")\n",
    "    \n",
    "\n",
    "    # identify subtasks\n",
    "    subtasks = {}\n",
    "    for e in examples:\n",
    "        subtask = e[subtask_key]\n",
    "        if subtask in subtasks:\n",
    "            subtasks[subtask].append(e)\n",
    "        else:\n",
    "            subtasks[subtask] = [e]\n",
    "\n",
    "    print(\"extracted subtasks\")\n",
    "\n",
    "    # for each subtask, compute top neurons and save\n",
    "    subtasks_neurons = {}\n",
    "    for subtask, examples in subtasks.items():\n",
    "        top_neurons = get_important_neurons(examples, batch_size, mlps, pad_len, mean_act_files, task=task, noimg=noimg)\n",
    "        subtasks_neurons[subtask] = top_neurons\n",
    "        print(f\"finished subtask: {subtask}\")\n",
    "\n",
    "    #with open(f\"data/{model_name}_e{epoch}_{task}_top_neurons_per_subtask.pkl\", \"wb\") as f:\n",
    "    #    pickle.dump(subtasks_neurons, f)\n",
    "    print(subtasks_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['git.embeddings.position_ids'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/\"\n",
    "spec = importlib.util.spec_from_file_location(\"GitForCausalLM\", f\"{model_path}modeling_git.py\")\n",
    "git_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"git_module\"] = git_module\n",
    "spec.loader.exec_module(git_module)\n",
    "GitForCausalLM = git_module.GitForCausalLM\n",
    "model2 = GitForCausalLM.from_pretrained(\"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/\") \n",
    "ckpt2 = torch.load(\"../babylm_GIT/models2/base_git_1vd125_s1/epoch17/pytorch_model.bin\") # TODO: newly initialized for vision encoder: ['pooler.dense.bias', 'pooler.dense.weight']\n",
    "model2.load_state_dict(ckpt2, strict=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded samples\n",
      "retrieved precomputed mean activations\n",
      "extracted subtasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished subtask: anaphor_agreement\n",
      "{'anaphor_agreement': {'mlp_0': (tensor([ 733, 1532,  900,  307, 2543, 1211,  972,   14, 2166,  951, 2654,  771,\n",
      "        1638,  784, 2890, 2193,  649, 1955, 1407, 1845, 3005,  389,  603, 1154,\n",
      "        1849,  707,  787,  671, 2675, 2167,  315, 2684,   28, 1799, 2283, 2104,\n",
      "         978,  781, 2781, 1311, 1354,   79, 2579, 2949,  196, 2236, 2709, 1668,\n",
      "        1067, 1636, 1403,  643,  280, 2740,  869,  153, 2258, 3010, 2408, 2530,\n",
      "        1204, 1404, 2540, 2593,  342, 1146,  500, 1339, 3049, 1814, 2417,   32,\n",
      "        2921,   81, 2918,  809, 1876, 2943,   71,  567,  662,  380,    5, 2101,\n",
      "         329, 1675, 2619, 1808, 2281,  204,  580,  145,  658, 2873,  666, 2713,\n",
      "          37, 2015, 1570, 1232]), tensor([0.5936, 0.4346, 0.3636, 0.3391, 0.3077, 0.2889, 0.2518, 0.2333, 0.2262,\n",
      "        0.2248, 0.2170, 0.2050, 0.2007, 0.1923, 0.1864, 0.1821, 0.1791, 0.1786,\n",
      "        0.1701, 0.1699, 0.1680, 0.1667, 0.1638, 0.1618, 0.1595, 0.1505, 0.1458,\n",
      "        0.1443, 0.1434, 0.1399, 0.1385, 0.1370, 0.1366, 0.1348, 0.1346, 0.1320,\n",
      "        0.1313, 0.1302, 0.1283, 0.1280, 0.1165, 0.1136, 0.1107, 0.1106, 0.1101,\n",
      "        0.1101, 0.1097, 0.1083, 0.1078, 0.1069, 0.1065, 0.1061, 0.1053, 0.1051,\n",
      "        0.1047, 0.1047, 0.1033, 0.1033, 0.1028, 0.1028, 0.1017, 0.1003, 0.0984,\n",
      "        0.0980, 0.0973, 0.0970, 0.0970, 0.0968, 0.0964, 0.0951, 0.0946, 0.0942,\n",
      "        0.0935, 0.0931, 0.0930, 0.0929, 0.0897, 0.0891, 0.0890, 0.0889, 0.0887,\n",
      "        0.0873, 0.0873, 0.0871, 0.0869, 0.0868, 0.0865, 0.0861, 0.0859, 0.0858,\n",
      "        0.0855, 0.0851, 0.0851, 0.0850, 0.0850, 0.0844, 0.0840, 0.0840, 0.0835,\n",
      "        0.0833])), 'mlp_1': (tensor([ 634, 2222,  168,  375,  308,  868,   17, 3068, 1140,  384,  954, 1777,\n",
      "         627, 2766, 1936, 2177, 1550, 1596,  364,  628, 2671, 3050, 1709, 2689,\n",
      "        1271,  167,  254,  190,  211, 2162,  689, 1234, 1735,  595, 1694,  902,\n",
      "         478, 1091, 2124,  855, 1511, 2014, 1137,  688, 3001, 2382, 1990, 2651,\n",
      "         233,  975,  619, 1491, 2103, 2538, 2457, 1065,  808, 1620,  342, 1513,\n",
      "          22, 2974,  760, 2578,  633, 1242, 2706, 1066, 1002, 1799, 2216, 3035,\n",
      "        1822, 2298,  800, 1495, 2053, 1508, 1359, 2408, 2003,  117,  473, 1030,\n",
      "         169,  916,   63,  906, 2723, 1941,  754,   81, 1646, 2339, 1996, 2018,\n",
      "        2223, 2146,  957,  423]), tensor([0.4327, 0.3295, 0.3015, 0.2281, 0.2183, 0.2141, 0.2121, 0.2102, 0.2080,\n",
      "        0.1959, 0.1823, 0.1756, 0.1700, 0.1697, 0.1682, 0.1657, 0.1615, 0.1595,\n",
      "        0.1565, 0.1518, 0.1457, 0.1456, 0.1412, 0.1412, 0.1389, 0.1385, 0.1370,\n",
      "        0.1368, 0.1357, 0.1353, 0.1307, 0.1296, 0.1273, 0.1263, 0.1260, 0.1243,\n",
      "        0.1241, 0.1239, 0.1238, 0.1224, 0.1223, 0.1215, 0.1188, 0.1182, 0.1168,\n",
      "        0.1154, 0.1150, 0.1139, 0.1132, 0.1124, 0.1111, 0.1104, 0.1078, 0.1071,\n",
      "        0.1059, 0.1036, 0.1004, 0.1002, 0.0997, 0.0993, 0.0986, 0.0984, 0.0980,\n",
      "        0.0975, 0.0971, 0.0968, 0.0968, 0.0967, 0.0964, 0.0962, 0.0952, 0.0949,\n",
      "        0.0923, 0.0918, 0.0912, 0.0910, 0.0910, 0.0906, 0.0905, 0.0901, 0.0898,\n",
      "        0.0889, 0.0888, 0.0882, 0.0881, 0.0880, 0.0875, 0.0874, 0.0874, 0.0874,\n",
      "        0.0867, 0.0864, 0.0854, 0.0854, 0.0852, 0.0847, 0.0841, 0.0840, 0.0836,\n",
      "        0.0834]))}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(\"blimp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"question_types.txt\", \"r\") as f1:\n",
    "    lines = f1.readlines()\n",
    "    qt = []\n",
    "    for l in lines:\n",
    "        qt.append(l.strip())\n",
    "\n",
    "with open(\"vqa_superclasses.txt\", \"r\") as f2:\n",
    "    lines = f2.readlines()\n",
    "    mapping = {}\n",
    "    for l in lines:\n",
    "        parts = l.split(\"-\")\n",
    "        mapping[parts[0].strip()] = parts[1].strip()\n",
    "\n",
    "for q in qt:\n",
    "    if q not in mapping:\n",
    "        print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_vqa_qtypes():\n",
    "    with open(\"vqa_superclasses.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        mapping = {}\n",
    "        for l in lines:\n",
    "            parts = l.split(\"-\")\n",
    "            mapping[parts[0].strip()] = parts[1].strip()\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is this': 'verification and existence',\n",
       " 'what is the': 'general queries and miscellaneous',\n",
       " 'do you': 'verification and existence',\n",
       " 'what': 'general queries and miscellaneous',\n",
       " 'what is': 'general queries and miscellaneous',\n",
       " 'can you': 'action and state',\n",
       " 'is the woman': 'person and object identification',\n",
       " 'what color is the': 'color identification',\n",
       " 'are the': 'verification and existence',\n",
       " 'is the': 'verification and existence',\n",
       " 'is this a': 'identification and classification',\n",
       " 'is it': 'verification and existence',\n",
       " 'what kind of': 'identification and classification',\n",
       " 'is the man': 'person and object identification',\n",
       " 'none of the above': 'general queries and miscellaneous',\n",
       " 'what color are the': 'color identification',\n",
       " 'what color': 'color identification',\n",
       " 'what sport is': 'identification and classification',\n",
       " 'was': 'temporal information',\n",
       " 'is there': 'verification and existence',\n",
       " 'is there a': 'verification and existence',\n",
       " 'are they': 'verification and existence',\n",
       " 'which': 'identification and classification',\n",
       " 'has': 'verification and existence',\n",
       " 'do': 'verification and existence',\n",
       " 'where is the': 'spatial and positional information',\n",
       " 'what type of': 'identification and classification',\n",
       " 'are there any': 'verification and existence',\n",
       " 'what does the': 'action and state',\n",
       " 'does this': 'verification and existence',\n",
       " 'is this an': 'identification and classification',\n",
       " 'is that a': 'identification and classification',\n",
       " 'what is this': 'identification and classification',\n",
       " 'are these': 'verification and existence',\n",
       " 'is': 'verification and existence',\n",
       " 'are': 'verification and existence',\n",
       " 'what is in the': 'spatial and positional information',\n",
       " 'what animal is': 'identification and classification',\n",
       " 'how': 'action and state',\n",
       " 'does the': 'verification and existence',\n",
       " 'are there': 'verification and existence',\n",
       " 'is this person': 'person and object identification',\n",
       " 'why': 'reason and purpose',\n",
       " 'what color is': 'color identification',\n",
       " 'what is the man': 'person and object identification',\n",
       " 'what time': 'temporal information',\n",
       " 'is he': 'person and object identification',\n",
       " 'what room is': 'spatial and positional information',\n",
       " 'what are': 'general queries and miscellaneous',\n",
       " 'what is the color of the': 'color identification',\n",
       " 'where are the': 'spatial and positional information',\n",
       " 'what are the': 'general queries and miscellaneous',\n",
       " 'what is on the': 'spatial and positional information',\n",
       " 'could': 'action and state',\n",
       " 'who is': 'person and object identification',\n",
       " 'is the person': 'person and object identification',\n",
       " 'what is the name': 'person and object identification',\n",
       " 'what is the person': 'person and object identification',\n",
       " 'what is the woman': 'person and object identification',\n",
       " 'how many': 'quantity and counting',\n",
       " 'what brand': 'identification and classification',\n",
       " 'why is the': 'reason and purpose'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = parse_vqa_qtypes()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alina/miniconda3/envs/babylm/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "hf_path = \"HuggingFaceM4/VQAv2\"\n",
    "hf_split = \"validation\"\n",
    "local_file = f\"data/vqa_filtered/vqa_distractors_info.json\"\n",
    "    \n",
    "hf_ds = load_dataset(hf_path)[hf_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types = set()\n",
    "for sample in hf_ds:\n",
    "    question_types.add(sample[\"question_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'are',\n",
       " 'are the',\n",
       " 'are there',\n",
       " 'are there any',\n",
       " 'are these',\n",
       " 'are they',\n",
       " 'can you',\n",
       " 'could',\n",
       " 'do',\n",
       " 'do you',\n",
       " 'does the',\n",
       " 'does this',\n",
       " 'has',\n",
       " 'how',\n",
       " 'how many',\n",
       " 'how many people are',\n",
       " 'how many people are in',\n",
       " 'is',\n",
       " 'is he',\n",
       " 'is it',\n",
       " 'is that a',\n",
       " 'is the',\n",
       " 'is the man',\n",
       " 'is the person',\n",
       " 'is the woman',\n",
       " 'is there',\n",
       " 'is there a',\n",
       " 'is this',\n",
       " 'is this a',\n",
       " 'is this an',\n",
       " 'is this person',\n",
       " 'none of the above',\n",
       " 'was',\n",
       " 'what',\n",
       " 'what animal is',\n",
       " 'what are',\n",
       " 'what are the',\n",
       " 'what brand',\n",
       " 'what color',\n",
       " 'what color are the',\n",
       " 'what color is',\n",
       " 'what color is the',\n",
       " 'what does the',\n",
       " 'what is',\n",
       " 'what is in the',\n",
       " 'what is on the',\n",
       " 'what is the',\n",
       " 'what is the color of the',\n",
       " 'what is the man',\n",
       " 'what is the name',\n",
       " 'what is the person',\n",
       " 'what is the woman',\n",
       " 'what is this',\n",
       " 'what kind of',\n",
       " 'what number is',\n",
       " 'what room is',\n",
       " 'what sport is',\n",
       " 'what time',\n",
       " 'what type of',\n",
       " 'where are the',\n",
       " 'where is the',\n",
       " 'which',\n",
       " 'who is',\n",
       " 'why',\n",
       " 'why is the'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
